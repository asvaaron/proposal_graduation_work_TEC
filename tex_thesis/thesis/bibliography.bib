
@article{mollahosseini_affectnet_2019,
	title = {{AffectNet}: {A} {Database} for {Facial} {Expression}, {Valence}, and {Arousal} {Computing} in the {Wild}},
	volume = {10},
	issn = {1949-3045, 2371-9850},
	shorttitle = {{AffectNet}},
	url = {http://arxiv.org/abs/1708.03985},
	doi = {10.1109/TAFFC.2017.2740923},
	abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
	number = {1},
	urldate = {2024-05-22},
	journal = {IEEE Transactions on Affective Computing},
	author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
	month = jan,
	year = {2019},
	note = {arXiv:1708.03985 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {18--31},
	annote = {Code implementation: https://github.com/jonathangiguere/Emotion\_Image\_Classifier
},
	annote = {Comment: IEEE Transactions on Affective Computing, 2017},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/9KM2DCAU/Mollahosseini et al. - 2019 - AffectNet A Database for Facial Expression, Valen.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/QPV3W95G/1708.html:text/html},
}

@misc{zhang_transformer-based_2022,
	title = {Transformer-based {Multimodal} {Information} {Fusion} for {Facial} {Expression} {Analysis}},
	url = {http://arxiv.org/abs/2203.12367},
	doi = {10.48550/arXiv.2203.12367},
	abstract = {Human affective behavior analysis has received much attention in human-computer interaction (HCI). In this paper, we introduce our submission to the CVPR 2022 Competition on Affective Behavior Analysis in-the-wild (ABAW). To fully exploit affective knowledge from multiple views, we utilize the multimodal features of spoken words, speech prosody, and facial expression, which are extracted from the video clips in the Aff-Wild2 dataset. Based on these features, we propose a unified transformer-based multimodal framework for Action Unit detection and also expression recognition. Specifically, the static vision feature is first encoded from the current frame image. At the same time, we clip its adjacent frames by a sliding window and extract three kinds of multimodal features from the sequence of images, audio, and text. Then, we introduce a transformer-based fusion module that integrates the static vision features and the dynamic multimodal features. The cross-attention module in the fusion module makes the output integrated features focus on the crucial parts that facilitate the downstream detection tasks. We also leverage some data balancing techniques, data augmentation techniques, and postprocessing methods to further improve the model performance. In the official test of ABAW3 Competition, our model ranks first in the EXPR and AU tracks. The extensive quantitative evaluations, as well as ablation studies on the Aff-Wild2 dataset, prove the effectiveness of our proposed method.},
	urldate = {2024-07-06},
	publisher = {arXiv},
	author = {Zhang, Wei and Qiu, Feng and Wang, Suzhen and Zeng, Hao and Zhang, Zhimeng and An, Rudong and Ma, Bowen and Ding, Yu},
	month = apr,
	year = {2022},
	note = {arXiv:2203.12367 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {Full Text:/Users/aaronmac/Zotero/storage/CKFJXCGC/Zhang et al. - 2022 - Transformer-based Multimodal Information Fusion fo.pdf:application/pdf},
}

@misc{saravanan_facial_2019,
	title = {Facial {Emotion} {Recognition} using {Convolutional} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1910.05602v1},
	abstract = {Facial expression recognition is a topic of great interest in most fields from artificial intelligence and gaming to marketing and healthcare. The goal of this paper is to classify images of human faces into one of seven basic emotions. A number of different models were experimented with, including decision trees and neural networks before arriving at a final Convolutional Neural Network (CNN) model. CNNs work better for image recognition tasks since they are able to capture spacial features of the inputs due to their large number of filters. The proposed model consists of six convolutional layers, two max pooling layers and two fully connected layers. Upon tuning of the various hyperparameters, this model achieved a final accuracy of 0.60.},
	language = {en},
	urldate = {2024-07-07},
	journal = {arXiv.org},
	author = {Saravanan, Akash and Perichetla, Gurudutt and Gayathri, Dr K. S.},
	month = oct,
	year = {2019},
	keywords = {notion},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/PSBK5G2E/Saravanan et al. - 2019 - Facial Emotion Recognition using Convolutional Neu.pdf:application/pdf},
}

@misc{kollias_affect_2021,
	title = {Affect {Analysis} in-the-wild: {Valence}-{Arousal}, {Expressions}, {Action} {Units} and a {Unified} {Framework}},
	shorttitle = {Affect {Analysis} in-the-wild},
	url = {http://arxiv.org/abs/2103.15792},
	doi = {10.48550/arXiv.2103.15792},
	abstract = {Affect recognition based on subjects' facial expressions has been a topic of major research in the attempt to generate machines that can understand the way subjects feel, act and react. In the past, due to the unavailability of large amounts of data captured in real-life situations, research has mainly focused on controlled environments. However, recently, social media and platforms have been widely used. Moreover, deep learning has emerged as a means to solve visual analysis and recognition problems. This paper exploits these advances and presents significant contributions for affect analysis and recognition in-the-wild. Affect analysis and recognition can be seen as a dual knowledge generation problem, involving: i) creation of new, large and rich in-the-wild databases and ii) design and training of novel deep neural architectures that are able to analyse affect over these databases and to successfully generalise their performance on other datasets. The paper focuses on large in-the-wild databases, i.e., Aff-Wild and Aff-Wild2 and presents the design of two classes of deep neural networks trained with these databases. The first class refers to uni-task affect recognition, focusing on prediction of the valence and arousal dimensional variables. The second class refers to estimation of all main behavior tasks, i.e. valence-arousal prediction; categorical emotion classification in seven basic facial expressions; facial Action Unit detection. A novel multi-task and holistic framework is presented which is able to jointly learn and effectively generalize and perform affect recognition over all existing in-the-wild databases. Large experimental studies illustrate the achieved performance improvement over the existing state-of-the-art in affect recognition.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Kollias, Dimitrios and Zafeiriou, Stefanos},
	month = mar,
	year = {2021},
	note = {arXiv:2103.15792 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
	annote = {Amazing state of the art paper },
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/WP4C5RHI/Kollias and Zafeiriou - 2021 - Affect Analysis in-the-wild Valence-Arousal, Expr.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/UU475Q8P/2103.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/NES9QTJ7/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/8IDZPETB/2010.html:text/html},
}

@misc{her_batch_2024,
	title = {Batch {Transformer}: {Look} for {Attention} in {Batch}},
	shorttitle = {Batch {Transformer}},
	url = {http://arxiv.org/abs/2407.04218},
	doi = {10.48550/arXiv.2407.04218},
	abstract = {Facial expression recognition (FER) has received considerable attention in computer vision, with "in-the-wild" environments such as human-computer interaction. However, FER images contain uncertainties such as occlusion, low resolution, pose variation, illumination variation, and subjectivity, which includes some expressions that do not match the target label. Consequently, little information is obtained from a noisy single image and it is not trusted. This could significantly degrade the performance of the FER task. To address this issue, we propose a batch transformer (BT), which consists of the proposed class batch attention (CBA) module, to prevent overfitting in noisy data and extract trustworthy information by training on features reflected from several images in a batch, rather than information from a single image. We also propose multi-level attention (MLA) to prevent overfitting the specific features by capturing correlations between each level. In this paper, we present a batch transformer network (BTN) that combines the above proposals. Experimental results on various FER benchmark datasets show that the proposed BTN consistently outperforms the state-ofthe-art in FER datasets. Representative results demonstrate the promise of the proposed BTN for FER.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Her, Myung Beom and Jeong, Jisu and Song, Hojoon and Han, Ji-Hyeong},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04218 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/23WRHVCF/Her et al. - 2024 - Batch Transformer Look for Attention in Batch.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/UH9LJEBL/2407.html:text/html},
}

@misc{ning_representation_2024,
	title = {Representation {Learning} and {Identity} {Adversarial} {Training} for {Facial} {Behavior} {Understanding}},
	url = {http://arxiv.org/abs/2407.11243},
	doi = {10.48550/arXiv.2407.11243},
	abstract = {Facial Action Unit (AU) detection has gained significant research attention as AUs contain complex expression information. In this paper, we unpack two fundamental factors in AU detection: data and subject identity regularization, respectively. Motivated by recent advances in foundation models, we highlight the importance of data and collect a diverse dataset Face9M, comprising 9 million facial images, from multiple public resources. Pretraining a masked autoencoder on Face9M yields strong performance in AU detection and facial expression tasks. We then show that subject identity in AU datasets provides a shortcut learning for the model and leads to sub-optimal solutions to AU predictions. To tackle this generic issue of AU tasks, we propose Identity Adversarial Training (IAT) and demonstrate that a strong IAT regularization is necessary to learn identity-invariant features. Furthermore, we elucidate the design space of IAT and empirically show that IAT circumvents the identity shortcut learning and results in a better solution. Our proposed methods, Facial Masked Autoencoder (FMAE) and IAT, are simple, generic and effective. Remarkably, the proposed FMAE-IAT approach achieves new state-of-the-art F1 scores on BP4D (67.1{\textbackslash}\%), BP4D+ (66.8{\textbackslash}\%), and DISFA (70.1{\textbackslash}\%) databases, significantly outperforming previous work. We release the code and model at https://github.com/forever208/FMAE-IAT, the first open-sourced facial model pretrained on 9 million diverse images.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Ning, Mang and Salah, Albert Ali and Ertugrul, Itir Onal},
	month = jul,
	year = {2024},
	note = {arXiv:2407.11243 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/B29YZRWL/Ning et al. - 2024 - Representation Learning and Identity Adversarial T.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/K5E3UAZ4/2407.html:text/html},
}

@article{valle_multi-task_2021,
	title = {Multi-task head pose estimation in-the-wild},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/2202.02299},
	doi = {10.1109/TPAMI.2020.3046323},
	abstract = {We present a deep learning-based multi-task approach for head pose estimation in images. We contribute with a network architecture and training strategy that harness the strong dependencies among face pose, alignment and visibility, to produce a top performing model for all three tasks. Our architecture is an encoder-decoder CNN with residual blocks and lateral skip connections. We show that the combination of head pose estimation and landmark-based face alignment significantly improve the performance of the former task. Further, the location of the pose task at the bottleneck layer, at the end of the encoder, and that of tasks depending on spatial information, such as visibility and alignment, in the final decoder layer, also contribute to increase the final performance. In the experiments conducted the proposed model outperforms the state-of-the-art in the face pose and visibility tasks. By including a final landmark regression step it also produces face alignment results on par with the state-of-the-art.},
	number = {8},
	urldate = {2024-08-05},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Valle, Roberto and Buenaposada, José Miguel and Baumela, Luis},
	month = aug,
	year = {2021},
	note = {arXiv:2202.02299 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {2874--2881},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/CPWBJE4M/Valle et al. - 2021 - Multi-task head pose estimation in-the-wild.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/V9SH8AE2/2202.html:text/html},
}

@article{zhang_dual-direction_2023,
	title = {A {Dual}-{Direction} {Attention} {Mixed} {Feature} {Network} for {Facial} {Expression} {Recognition}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/17/3595},
	doi = {10.3390/electronics12173595},
	abstract = {In recent years, facial expression recognition (FER) has garnered significant attention within the realm of computer vision research. This paper presents an innovative network called the Dual-Direction Attention Mixed Feature Network (DDAMFN) specifically designed for FER, boasting both robustness and lightweight characteristics. The network architecture comprises two primary components: the Mixed Feature Network (MFN) serving as the backbone, and the Dual-Direction Attention Network (DDAN) functioning as the head. To enhance the network’s capability in the MFN, resilient features are extracted by utilizing mixed-size kernels. Additionally, a new Dual-Direction Attention (DDA) head that generates attention maps in two orientations is proposed, enabling the model to capture long-range dependencies effectively. To further improve the accuracy, a novel attention loss mechanism for the DDAN is introduced with different heads focusing on distinct areas of the input. Experimental evaluations on several widely used public datasets, including AffectNet, RAF-DB, and FERPlus, demonstrate the superiority of the DDAMFN compared to other existing models, which establishes that the DDAMFN as the state-of-the-art model in the field of FER.},
	language = {en},
	number = {17},
	urldate = {2024-08-05},
	journal = {Electronics},
	author = {Zhang, Saining and Zhang, Yuhang and Zhang, Ye and Wang, Yufei and Song, Zhigang},
	month = jan,
	year = {2023},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {coordinate attention, facial expression recognition, MixConv, MobileFaceNets, notion},
	pages = {3595},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/S2MYJ339/Zhang et al. - 2023 - A Dual-Direction Attention Mixed Feature Network f.pdf:application/pdf},
}

@misc{wang_region_2019,
	title = {Region {Attention} {Networks} for {Pose} and {Occlusion} {Robust} {Facial} {Expression} {Recognition}},
	url = {http://arxiv.org/abs/1905.04075},
	doi = {10.48550/arXiv.1905.04075},
	abstract = {Occlusion and pose variations, which can change facial appearance significantly, are two major obstacles for automatic Facial Expression Recognition (FER). Though automatic FER has made substantial progresses in the past few decades, occlusion-robust and pose-invariant issues of FER have received relatively less attention, especially in real-world scenarios. This paper addresses the real-world pose and occlusion robust FER problem with three-fold contributions. First, to stimulate the research of FER under real-world occlusions and variant poses, we build several in-the-wild facial expression datasets with manual annotations for the community. Second, we propose a novel Region Attention Network (RAN), to adaptively capture the importance of facial regions for occlusion and pose variant FER. The RAN aggregates and embeds varied number of region features produced by a backbone convolutional neural network into a compact fixed-length representation. Last, inspired by the fact that facial expressions are mainly defined by facial action units, we propose a region biased loss to encourage high attention weights for the most important regions. We validate our RAN and region biased loss on both our built test datasets and four popular datasets: FERPlus, AffectNet, RAF-DB, and SFEW. Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose. Our method also achieves state-of-the-art results on FERPlus, AffectNet, RAF-DB, and SFEW. Code and the collected test data will be publicly available.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Wang, Kai and Peng, Xiaojiang and Yang, Jianfei and Meng, Debin and Qiao, Yu},
	month = sep,
	year = {2019},
	note = {arXiv:1905.04075 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	annote = {Comment: The test set and the code of this paper will be available at https://github.com/kaiwang960112/Challenge-condition-FER-dataset},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/4JEPHGFB/Wang et al. - 2019 - Region Attention Networks for Pose and Occlusion R.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/V9W38XGB/1905.html:text/html},
}

@article{ma_facial_2023,
	title = {Facial {Expression} {Recognition} with {Visual} {Transformers} and {Attentional} {Selective} {Fusion}},
	volume = {14},
	issn = {1949-3045, 2371-9850},
	url = {http://arxiv.org/abs/2103.16854},
	doi = {10.1109/TAFFC.2021.3122146},
	abstract = {Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues deﬁnitely increase the difﬁculty of FER on account of these information-deﬁcient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging two kinds of feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with the global-local attention. The fused feature maps are then ﬂattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14\%, FERPlus with 88.81\% and AffectNet with 61.85\%. The cross-dataset evaluation on CK+ shows the promising generalization capability of the proposed method.},
	language = {en},
	number = {2},
	urldate = {2024-08-17},
	journal = {IEEE Transactions on Affective Computing},
	author = {Ma, Fuyan and Sun, Bin and Li, Shutao},
	month = apr,
	year = {2023},
	note = {arXiv:2103.16854 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {1236--1248},
	file = {Ma et al. - 2023 - Facial Expression Recognition with Visual Transfor.pdf:/Users/aaronmac/Zotero/storage/HSRGVYFV/Ma et al. - 2023 - Facial Expression Recognition with Visual Transfor.pdf:application/pdf},
}

@inproceedings{zhao_former-dfer_2021,
	address = {New York, NY, USA},
	series = {{MM} '21},
	title = {Former-{DFER}: {Dynamic} {Facial} {Expression} {Recognition} {Transformer}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {Former-{DFER}},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475292},
	doi = {10.1145/3474085.3475292},
	abstract = {This paper proposes a dynamic facial expression recognition transformer (Former-DFER) for the in-the-wild scenario. Specifically, the proposed Former-DFER mainly consists of a convolutional spatial transformer (CS-Former) and a temporal transformer (T-Former). The CS-Former consists of five convolution blocks and N spatial encoders, which is designed to guide the network to learn occlusion and pose-robust facial features from the spatial perspective. And the temporal transformer consists of M temporal encoders, which is designed to allow the network to learn contextual facial features from the temporal perspective. The heatmaps of the leaned facial features demonstrate that the proposed Former-DFER is capable of handling the issues such as occlusion, non-frontal pose, and head motion. And the visualization of the feature distribution shows that the proposed method can learn more discriminative facial features. Moreover, our Former-DFER also achieves state-of-the-art results on the DFEW and AFEW benchmarks.},
	urldate = {2024-08-20},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Zengqun and Liu, Qingshan},
	month = oct,
	year = {2021},
	keywords = {notion},
	pages = {1553--1561},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/ILC9RHKA/Zhao and Liu - 2021 - Former-DFER Dynamic Facial Expression Recognition.pdf:application/pdf},
}

@misc{zheng_poster_2022,
	title = {{POSTER}: {A} {Pyramid} {Cross}-{Fusion} {Transformer} {Network} for {Facial} {Expression} {Recognition}},
	shorttitle = {{POSTER}},
	url = {https://arxiv.org/abs/2204.04083v2},
	abstract = {Facial expression recognition (FER) is an important task in computer vision, having practical applications in areas such as human-computer interaction, education, healthcare, and online monitoring. In this challenging FER task, there are three key issues especially prevalent: inter-class similarity, intra-class discrepancy, and scale sensitivity. While existing works typically address some of these issues, none have fully addressed all three challenges in a unified framework. In this paper, we propose a two-stream Pyramid crOss-fuSion TransformER network (POSTER), that aims to holistically solve all three issues. Specifically, we design a transformer-based cross-fusion method that enables effective collaboration of facial landmark features and image features to maximize proper attention to salient facial regions. Furthermore, POSTER employs a pyramid structure to promote scale invariance. Extensive experimental results demonstrate that our POSTER achieves new state-of-the-art results on RAF-DB (92.05\%), FERPlus (91.62\%), as well as AffectNet 7 class (67.31\%) and 8 class (63.34\%). The code is available at https://github.com/zczcwh/POSTER.},
	language = {en},
	urldate = {2024-08-22},
	journal = {arXiv.org},
	author = {Zheng, Ce and Mendieta, Matias and Chen, Chen},
	month = apr,
	year = {2022},
	keywords = {notion},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/HX9KSCNJ/Zheng et al. - 2022 - POSTER A Pyramid Cross-Fusion Transformer Network.pdf:application/pdf},
}

@inproceedings{li_multi-branch_2023,
	address = {Singapore, Singapore},
	title = {Multi-branch {Attention} {Consistency} {Network} for {Facial} {Expression} {Recognition}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350331820},
	url = {https://ieeexplore.ieee.org/document/10312661/},
	doi = {10.1109/IECON51785.2023.10312661},
	abstract = {Due to the high inter-class similarity and subjective annotation of facial expressions, annotation uncertainty has become the key challenge in recent years. In this paper, we propose a Multi-branch Attention Consistency Network for facial expression recognition by combining latent label distribution learning and attention consistency to alleviate the annotation uncertainty. To be specific, we design three modules, namely multi-branch feature classification (MFC), multi-branch latent distribution learning (MLD) and multi-class attention consistency (MAC). The MFC classifies uncertain expressions through multiple auxiliary branches, which obtains attention maps and the degree of confidence for different facial categories. The MLD guides the target branch to learn latent label distributions from auxiliary branches. The MAC learns attention regions by multi-class attention consistency between auxiliary and target branches. Finally, we demonstrate the effectiveness of our proposed method by conducting experiments on three popular facial expression datasets. Experimental results show that our method achieves the state-of-the-art results of 90.16\%, 89.98\%, 63.12\% accuracy on RAF-DB, FERPlus and AffectNet datasets, respectively.},
	language = {en},
	urldate = {2024-08-22},
	booktitle = {{IECON} 2023- 49th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	publisher = {IEEE},
	author = {Li, Jing and Hu, Tianyu and Ouyang, Gaoxiang},
	month = oct,
	year = {2023},
	keywords = {notion},
	pages = {1--6},
	file = {Li et al. - 2023 - Multi-branch Attention Consistency Network for Fac.pdf:/Users/aaronmac/Zotero/storage/WLJD82CC/Li et al. - 2023 - Multi-branch Attention Consistency Network for Fac.pdf:application/pdf},
}

@misc{wu_-net_2023,
	title = {{LA}-{Net}: {Landmark}-{Aware} {Learning} for {Reliable} {Facial} {Expression} {Recognition} under {Label} {Noise}},
	shorttitle = {{LA}-{Net}},
	url = {http://arxiv.org/abs/2307.09023},
	doi = {10.48550/arXiv.2307.09023},
	abstract = {Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions. The derived noisy labels significantly harm the performance in real-world scenarios. To address this issue, we present a new FER model named Landmark-Aware Net{\textasciitilde}(LA-Net), which leverages facial landmarks to mitigate the impact of label noise from two perspectives. Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distribution of each sample by neighborhood aggregation, which in turn improves the quality of training supervision. Secondly, the model incorporates landmark information into expression representations using the devised expression-landmark contrastive loss. The enhanced expression feature extractor can be less susceptible to label noise. Our method can be integrated with any deep neural network for better training supervision without introducing extra inference costs. We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net achieves state-of-the-art performance.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Wu, Zhiyu and Cui, Jinshi},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09023 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	annote = {Comment: accepted by ICCV 2023},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/WXDGJAXU/Wu and Cui - 2023 - LA-Net Landmark-Aware Learning for Reliable Facia.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/PIYIA8RT/2307.html:text/html},
}

@misc{zhang_leave_2023,
	title = {Leave {No} {Stone} {Unturned}: {Mine} {Extra} {Knowledge} for {Imbalanced} {Facial} {Expression} {Recognition}},
	shorttitle = {Leave {No} {Stone} {Unturned}},
	url = {http://arxiv.org/abs/2310.19636},
	doi = {10.48550/arXiv.2310.19636},
	abstract = {Facial expression data is characterized by a significant imbalance, with most collected data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our aim is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by that, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Zhang, Yuhang and Li, Yaqi and Qin, Lixiong and Liu, Xuannan and Deng, Weihong},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19636 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	annote = {Comment: Accepted by NeurIPS2023},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/B3MHJXKN/Zhang et al. - 2023 - Leave No Stone Unturned Mine Extra Knowledge for .pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/BDY5DYUX/2310.html:text/html},
}

@misc{mao_poster_2023,
	title = {{POSTER}++: {A} simpler and stronger facial expression recognition network},
	shorttitle = {{POSTER}++},
	url = {http://arxiv.org/abs/2301.12149},
	doi = {10.48550/arXiv.2301.12149},
	abstract = {Facial expression recognition (FER) plays an important role in a variety of real-world applications such as human-computer interaction. POSTER achieves the state-of-the-art (SOTA) performance in FER by effectively combining facial landmark and image features through two-stream pyramid cross-fusion design. However, the architecture of POSTER is undoubtedly complex. It causes expensive computational costs. In order to relieve the computational pressure of POSTER, in this paper, we propose POSTER++. It improves POSTER in three directions: cross-fusion, two-stream, and multi-scale feature extraction. In cross-fusion, we use window-based cross-attention mechanism replacing vanilla cross-attention mechanism. We remove the image-to-landmark branch in the two-stream design. For multi-scale feature extraction, POSTER++ combines images with landmark's multi-scale features to replace POSTER's pyramid design. Extensive experiments on several standard datasets show that our POSTER++ achieves the SOTA FER performance with the minimum computational cost. For example, POSTER++ reached 92.21\% on RAF-DB, 67.49\% on AffectNet (7 cls) and 63.77\% on AffectNet (8 cls), respectively, using only 8.4G floating point operations (FLOPs) and 43.7M parameters (Param). This demonstrates the effectiveness of our improvements.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Mao, Jiawei and Xu, Rui and Yin, Xuesong and Chang, Yuanqi and Nie, Binling and Huang, Aibin},
	month = feb,
	year = {2023},
	note = {arXiv:2301.12149 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/PIRW32MZ/Mao et al. - 2023 - POSTER++ A simpler and stronger facial expression.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/77XHUI4N/2301.html:text/html},
}

@misc{shi_learning_2021,
	title = {Learning to {Amend} {Facial} {Expression} {Representation} via {De}-albino and {Affinity}},
	url = {http://arxiv.org/abs/2103.10189},
	doi = {10.48550/arXiv.2103.10189},
	abstract = {Facial Expression Recognition (FER) is a classification task that points to face variants. Hence, there are certain affinity features between facial expressions, receiving little attention in the FER literature. Convolution padding, despite helping capture the edge information, causes erosion of the feature map simultaneously. After multi-layer filling convolution, the output feature map named albino feature definitely weakens the representation of the expression. To tackle these challenges, we propose a novel architecture named Amending Representation Module (ARM). ARM is a substitute for the pooling layer. Theoretically, it can be embedded in the back end of any network to deal with the Padding Erosion. ARM efficiently enhances facial expression representation from two different directions: 1) reducing the weight of eroded features to offset the side effect of padding, and 2) decomposing facial features to simplify representation learning. Experiments on public benchmarks prove that our ARM boosts the performance of FER remarkably. The validation accuracies are respectively 90.42\% on RAF-DB, 65.2\% on Affect-Net, and 58.71\% on SFEW, exceeding current state-of-the-art methods. Our implementation and trained models are available at https://github.com/JiaweiShiCV/Amend-Representation-Module.},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Shi, Jiawei and Zhu, Songhao and Liang, Zhiwei},
	month = oct,
	year = {2021},
	note = {arXiv:2103.10189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/TL2DFU2K/Shi et al. - 2021 - Learning to Amend Facial Expression Representation.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/76MJI2MQ/2103.html:text/html},
}

@misc{wang_survey_2024,
	title = {A {Survey} on {Facial} {Expression} {Recognition} of {Static} and {Dynamic} {Emotions}},
	url = {http://arxiv.org/abs/2408.15777},
	doi = {10.48550/arXiv.2408.15777},
	abstract = {Facial expression recognition (FER) aims to analyze emotional states from static images and dynamic sequences, which is pivotal in enhancing anthropomorphic communication among humans, robots, and digital avatars by leveraging AI technologies. As the FER field evolves from controlled laboratory environments to more complex in-the-wild scenarios, advanced methods have been rapidly developed and new challenges and apporaches are encounted, which are not well addressed in existing reviews of FER. This paper offers a comprehensive survey of both image-based static FER (SFER) and video-based dynamic FER (DFER) methods, analyzing from model-oriented development to challenge-focused categorization. We begin with a critical comparison of recent reviews, an introduction to common datasets and evaluation criteria, and an in-depth workflow on FER to establish a robust research foundation. We then systematically review representative approaches addressing eight main challenges in SFER (such as expression disturbance, uncertainties, compound emotions, and cross-domain inconsistency) as well as seven main challenges in DFER (such as key frame sampling, expression intensity variations, and cross-modal alignment). Additionally, we analyze recent advancements, benchmark performances, major applications, and ethical considerations. Finally, we propose five promising future directions and development trends to guide ongoing research. The project page for this paper can be found at https://github.com/wangyanckxx/SurveyFER.},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Wang, Yan and Yan, Shaoqi and Liu, Yang and Song, Wei and Liu, Jing and Chang, Yang and Mai, Xinji and Hu, Xiping and Zhang, Wenqiang and Gan, Zhongxue},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15777 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/PBPBXXQJ/Wang et al. - 2024 - A Survey on Facial Expression Recognition of Stati.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/5Y2NC62I/2408.html:text/html},
}

@misc{islam_recent_2023,
	title = {Recent {Advances} in {Vision} {Transformer}: {A} {Survey} and {Outlook} of {Recent} {Work}},
	shorttitle = {Recent {Advances} in {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2203.01536},
	doi = {10.48550/arXiv.2203.01536},
	abstract = {Vision Transformers (ViTs) are becoming more popular and dominating technique for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a demanding technique in computer vision, ViTs have been successfully solved various vision problems while focusing on long-range relationships. In this paper, we begin by introducing the fundamental concepts and background of the self-attention mechanism. Next, we provide a comprehensive overview of recent top-performing ViT methods describing in terms of strength and weakness, computational cost as well as training and testing dataset. We thoroughly compare the performance of various ViT algorithms and most representative CNN methods on popular benchmark datasets. Finally, we explore some limitations with insightful observations and provide further research direction. The project page along with the collections of papers are available at https://github.com/khawar512/ViT-Survey},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Islam, Khawar},
	month = oct,
	year = {2023},
	note = {arXiv:2203.01536 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, notion},
	annote = {Comment: Added AAAI 2022 methods and working on ICLR 2022 methods and ICML 2022},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/8F28AM4I/Islam - 2023 - Recent Advances in Vision Transformer A Survey an.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/DL3YKFFG/2203.html:text/html},
}

@article{leon-sarkis_alisis_2017,
	title = {Un análisis comparativo de los algoritmos {Fast} {Radial} {Symmetry} {Transform} y {Hough} {Transform} para la detección automática de granos de café en imágenes},
	url = {https://repositoriotec.tec.ac.cr/handle/2238/9372},
	abstract = {In this work we present a strategy that contributes to the overall solution of a 
problem presented by the Costa Rica Co ee Institute (ICAFE). ICAFE owns a set 
of coffee grains images and needs to  nd an automatic way, through computer 
vision, to detect and count the number of grains in each image in order to increase 
the e ciency in the process of estimating yield. 
A strategy to detect co ee grains in images is proposed, by combining the 
algorithms Fast Radial Symmetry Transform[8] and Hough Transform[19]. Then, 
this strategy is incorporated in the grain detection process of P-TRAP[13], an 
open-source tool, to increase the precision in the detection of existing coffee grains. 
The images are taken with a mobile device in a non-controlled environment in which 
the grains are not pulled o  their natural environment. 
Likewise, a comparative analisis is done between the P-TRAP version developed in 
this study and both algorithms running individually. The number of existing grains 
in an image is determined manually. Then, the cherry detection process is executed 
over each image and results are collected. Finally, a detailed analisis is done over 
the results obtained.},
	language = {spa},
	urldate = {2024-10-07},
	author = {León-Sarkis, Marco},
	year = {2017},
	note = {Accepted: 2018-02-08T21:53:34Z
Publisher: Instituto Tecnológico de Costa Rica},
	keywords = {notion},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/55R3297X/León-Sarkis - 2017 - Un análisis comparativo de los algoritmos Fast Rad.pdf:application/pdf},
}

@misc{noauthor_real-world_nodate,
	title = {Real-world {Affective} {Faces} ({RAF}) {Database}},
	url = {http://www.whdeng.cn/raf/model1.html},
	urldate = {2024-10-12},
	keywords = {notion},
	file = {Real-world Affective Faces (RAF) Database:/Users/aaronmac/Zotero/storage/CMQTIDD6/model1.html:text/html},
}

@misc{noauthor_facial_nodate,
	title = {Facial {Expression} {Recognition} with {Visual} {Transformers} and {Attentional} {Selective} {Fusion}},
	url = {https://ar5iv.labs.arxiv.org/html/2103.16854},
	abstract = {Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions.
Although substantial progresses have been…},
	language = {en},
	urldate = {2024-10-24},
	journal = {ar5iv},
	keywords = {notion},
	file = {Snapshot:/Users/aaronmac/Zotero/storage/2V57HG5D/2103.html:text/html},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {RAF}-{DB} {Dataset}},
	url = {https://paperswithcode.com/dataset/raf-db},
	abstract = {The Real-world Affective Faces Database (RAF-DB) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.},
	language = {en},
	urldate = {2024-10-26},
	keywords = {notion},
	file = {Snapshot:/Users/aaronmac/Zotero/storage/YMFJT7HU/raf-db.html:text/html},
}

@inproceedings{ilikci_heat-map_2019,
	title = {Heat-{Map} {Based} {Emotion} and {Face} {Recognition} from {Thermal} {Images}},
	url = {https://ieeexplore.ieee.org/document/9018786},
	doi = {10.1109/ComComAp46287.2019.9018786},
	abstract = {Nowadays emotion recognition becomes feasible in the Computer Vision domain with the help of Convolutional Neural Networks. However, the credibility of emotion recognition from daily images or videos is evidently insufficient. As people can easily mimic emotions one after another by fooling the computational models, different defensive approaches should be taken into consideration. Particularly, thermal images taken by thermal cameras visualize the facial and body's heat status, revealing where humans actually feel emotions; therefore, models trained with thermal heat-maps are less subject to fake expressions. Accordingly, heat-maps provide suitable resources for developing more credible emotion recognition models. In this paper, a fast detection algorithm, YOLO, is adapted and trained to detect emotions in thermal images. The detection performance, in terms of average precision and intersection over union, from three detection algorithms, YOLO, ResNet, and DenseNet, is compared and their respective characteristics are discussed.},
	urldate = {2024-10-28},
	booktitle = {2019 {Computing}, {Communications} and {IoT} {Applications} ({ComComAp})},
	author = {Ilikci, Burak and Chen, Lei and Cho, Hyuk and Liu, Qingzhong},
	month = oct,
	year = {2019},
	keywords = {Databases, notion, Cameras, Convolutional Neural Network, Emotion recognition, Face, Infrared heating, Principal component analysis, Thermal image, YOLOv3},
	pages = {449--453},
}

@misc{duan_qafe-net_2023,
	title = {{QAFE}-{Net}: {Quality} {Assessment} of {Facial} {Expressions} with {Landmark} {Heatmaps}},
	shorttitle = {{QAFE}-{Net}},
	url = {http://arxiv.org/abs/2312.00856},
	doi = {10.48550/arXiv.2312.00856},
	abstract = {Facial expression recognition (FER) methods have made great inroads in categorising moods and feelings in humans. Beyond FER, pain estimation methods assess levels of intensity in pain expressions, however assessing the quality of all facial expressions is of critical value in health-related applications. In this work, we address the quality of five different facial expressions in patients affected by Parkinson's disease. We propose a novel landmark-guided approach, QAFE-Net, that combines temporal landmark heatmaps with RGB data to capture small facial muscle movements that are encoded and mapped to severity scores. The proposed approach is evaluated on a new Parkinson's Disease Facial Expression dataset (PFED5), as well as on the pain estimation benchmark, the UNBC-McMaster Shoulder Pain Expression Archive Database. Our comparative experiments demonstrate that the proposed method outperforms SOTA action quality assessment works on PFED5 and achieves lower mean absolute error than the SOTA pain estimation methods on UNBC-McMaster. Our code and the new PFED5 dataset are available at https://github.com/shuchaoduan/QAFE-Net.},
	urldate = {2024-10-28},
	publisher = {arXiv},
	author = {Duan, Shuchao and Dadashzadeh, Amirhossein and Whone, Alan and Mirmehdi, Majid},
	month = dec,
	year = {2023},
	note = {arXiv:2312.00856 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	annote = {Comment: Accepted to ELFA workshop at WACV 2024},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/T8FJDEK3/Duan et al. - 2023 - QAFE-Net Quality Assessment of Facial Expressions.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/89FYXWRJ/2312.html:text/html},
}

@inproceedings{faizabadi_exploring_2023,
	title = {Exploring {Robust} {Pose} {Invariant} {Face} {Recognition} with {Vision} {Transformers}: {A} {Multi}-{Modal} {Study}},
	shorttitle = {Exploring {Robust} {Pose} {Invariant} {Face} {Recognition} with {Vision} {Transformers}},
	url = {https://ieeexplore.ieee.org/document/10346325},
	doi = {10.1109/ICETAS59148.2023.10346325},
	abstract = {This paper investigates the application of vision transformers (ViT) to face recognition tasks, focusing on their robustness against pose variations in multi-modal settings. Leveraging the advantages of ViT, such as global context modeling and attention mechanisms, we evaluate their performance using RGB and depth face modalities. To facilitate this investigation, we propose a new multi-view and multi-modal dataset, featuring various pose variations, ranging from slight to extreme, with incorporated facial expressions. Four robust pose invariant evaluation protocols are established for comprehensive analysis. Three variants of ViT models (Base, Small, and Tiny) are trained and evaluated. Our findings indicate that ViT-Base achieves 97.11 \% accuracy for RGB faces, while ViT-Small outperforms all RGB Face Transformers in depth modality, achieving 99.02\% accuracy across all pose protocols. Importantly, our empirical study highlights the enhanced robustness of 3D depth maps for pose invariant face recognition tasks, irrespective of the deep learning techniques employed. These insights contribute to the understanding of ViT models in face recognition and open avenues for future research in this domain.},
	urldate = {2024-11-19},
	booktitle = {2023 {IEEE} 8th {International} {Conference} on {Engineering} {Technologies} and {Applied} {Sciences} ({ICETAS})},
	author = {Faizabadi, Ahmed Rimaz and Mohammed Shweesh, Osamah Ebrahim and Mohd Zaki, Hasan Firdaus and Mohammed, Abdulrahman Ibraheem and Khan, Mohammad Amir and Aliman, Norazam},
	month = oct,
	year = {2023},
	note = {ISSN: 2769-4518},
	keywords = {Deep learning, Face recognition, notion, 3D Face Dataset, Face Recognition, Focusing, Pose Invariant, Protocols, Solid modeling, Three-dimensional displays, Transformers, Vision Transformer},
	pages = {1--7},
}

@misc{ranjan_light-weight_2018,
	title = {Light-weight {Head} {Pose} {Invariant} {Gaze} {Tracking}},
	url = {http://arxiv.org/abs/1804.08572},
	doi = {10.48550/arXiv.1804.08572},
	abstract = {Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor.},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Ranjan, Rajeev and Mello, Shalini De and Kautz, Jan},
	month = apr,
	year = {2018},
	note = {arXiv:1804.08572},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/ITJC8EQW/Ranjan et al. - 2018 - Light-weight Head Pose Invariant Gaze Tracking.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/YZYM6FVZ/1804.html:text/html},
}

@misc{marcu_pitfalls_2022,
	title = {On {Pitfalls} of {Measuring} {Occlusion} {Robustness} through {Data} {Distortion}},
	url = {http://arxiv.org/abs/2211.13734},
	doi = {10.48550/arXiv.2211.13734},
	abstract = {Over the past years, the crucial role of data has largely been shadowed by the field's focus on architectures and training procedures. We often cause changes to the data without being aware of their wider implications. In this paper we show that distorting images without accounting for the artefacts introduced leads to biased results when establishing occlusion robustness. To ensure models behave as expected in real-world scenarios, we need to rule out the impact added artefacts have on evaluation. We propose a new approach, iOcclusion, as a fairer alternative for applications where the possible occluders are unknown.},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Marcu, Antonia},
	month = nov,
	year = {2022},
	note = {arXiv:2211.13734},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/J2AIX5QY/Marcu - 2022 - On Pitfalls of Measuring Occlusion Robustness thro.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/Y9KSB4N7/2211.html:text/html},
}

@misc{guo_occrob_2023,
	title = {{OccRob}: {Efficient} {SMT}-{Based} {Occlusion} {Robustness} {Verification} of {Deep} {Neural} {Networks}},
	shorttitle = {{OccRob}},
	url = {http://arxiv.org/abs/2301.11912},
	doi = {10.48550/arXiv.2301.11912},
	abstract = {Occlusion is a prevalent and easily realizable semantic perturbation to deep neural networks (DNNs). It can fool a DNN into misclassifying an input image by occluding some segments, possibly resulting in severe errors. Therefore, DNNs planted in safety-critical systems should be verified to be robust against occlusions prior to deployment. However, most existing robustness verification approaches for DNNs are focused on non-semantic perturbations and are not suited to the occlusion case. In this paper, we propose the first efficient, SMT-based approach for formally verifying the occlusion robustness of DNNs. We formulate the occlusion robustness verification problem and prove it is NP-complete. Then, we devise a novel approach for encoding occlusions as a part of neural networks and introduce two acceleration techniques so that the extended neural networks can be efficiently verified using off-the-shelf, SMT-based neural network verification tools. We implement our approach in a prototype called OccRob and extensively evaluate its performance on benchmark datasets with various occlusion variants. The experimental results demonstrate our approach's effectiveness and efficiency in verifying DNNs' robustness against various occlusions, and its ability to generate counterexamples when these DNNs are not robust.},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Guo, Xingwu and Zhou, Ziwei and Zhang, Yueling and Katz, Guy and Zhang, Min},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11912},
	keywords = {Computer Science - Machine Learning, notion},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/RJXWMNJC/Guo et al. - 2023 - OccRob Efficient SMT-Based Occlusion Robustness V.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/PQG5L4P2/2301.html:text/html},
}

@article{peng_facial_2024,
	title = {Facial {Expression} {Recognition}-{You} {Only} {Look} {Once}-{Neighborhood} {Coordinate} {Attention} {Mamba}: {Facial} {Expression} {Detection} and {Classification} {Based} on {Neighbor} and {Coordinates} {Attention} {Mechanism}},
	volume = {24},
	issn = {1424-8220},
	shorttitle = {Facial {Expression} {Recognition}-{You} {Only} {Look} {Once}-{Neighborhood} {Coordinate} {Attention} {Mamba}},
	doi = {10.3390/s24216912},
	abstract = {In studying the joint object detection and classification problem for facial expression recognition (FER) deploying the YOLOX framework, we introduce a novel feature extractor, called neighborhood coordinate attention Mamba (NCAMamba) to substitute for the original feature extractor in the Feature Pyramid Network (FPN). NCAMamba combines the background information reduction capabilities of Mamba, the local neighborhood relationship understanding of neighborhood attention, and the directional relationship understanding of coordinate attention. The resulting FER-YOLO-NCAMamba model, when applied to two unaligned FER benchmark datasets, RAF-DB and SFEW, obtains significantly improved mean average precision (mAP) scores when compared with those obtained by other state-of-the-art methods. Moreover, in ablation studies, it is found that the NCA module is relatively more important than the Visual State Space (VSS), a version of using Mamba for image processing, and in visualization studies using the grad-CAM method, it reveals that regions around the nose tip are critical to recognizing the expression; if it is too large, it may lead to erroneous prediction, while a small focused region would lead to correct recognition; this may explain why FER of unaligned faces is such a challenging problem.},
	language = {eng},
	number = {21},
	journal = {Sensors (Basel, Switzerland)},
	author = {Peng, Cheng and Sun, Mingqi and Zou, Kun and Zhang, Bowen and Dai, Genan and Tsoi, Ah Chung},
	month = oct,
	year = {2024},
	pmid = {39517809},
	pmcid = {PMC11548711},
	keywords = {facial expression recognition, notion, Algorithms, attention, Attention, Automated Facial Recognition, Facial Expression, Facial Recognition, Humans, Image Processing, Computer-Assisted, object detection, Pattern Recognition, Automated, visual state space model},
	pages = {6912},
}

@misc{abdullah_activator_2024,
	title = {Activator: {GLU} {Activation} {Function} as the {Core} {Component} of a {Vision} {Transformer}},
	shorttitle = {Activator},
	url = {http://arxiv.org/abs/2405.15953},
	doi = {10.48550/arXiv.2405.15953},
	abstract = {Transformer architecture currently represents the main driver behind many successes in a variety of tasks addressed by deep learning, especially the recent advances in natural language processing (NLP) culminating with large language models (LLM). In addition, transformer architecture has found a wide spread of interest from computer vision (CV) researchers and practitioners, allowing for many advancements in vision-related tasks and opening the door for multi-task and multi-modal deep learning architectures that share the same principle of operation. One drawback to these architectures is their reliance on the scaled dot product attention mechanism with the softmax activation function, which is computationally expensive and requires large compute capabilities both for training and inference. This paper investigates substituting the attention mechanism usually adopted for transformer architecture with an architecture incorporating gated linear unit (GLU) activation within a multi-layer perceptron (MLP) structure in conjunction with the default MLP incorporated in the traditional transformer design. Another step forward taken by this paper is to eliminate the second non-gated MLP to further reduce the computational cost. Experimental assessments conducted by this research show that both proposed modifications and reductions offer competitive performance in relation to baseline architectures, in support of the aims of this work in establishing a more efficient yet capable alternative to the traditional attention mechanism as the core component in designing transformer architectures.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Abdullah, Abdullah Nazhat and Aydin, Tarkan},
	month = aug,
	year = {2024},
	note = {arXiv:2405.15953 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:2403.02411},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/3MJG9HAT/Abdullah and Aydin - 2024 - Activator GLU Activation Function as the Core Com.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/GYVFQEGQ/2405.html:text/html},
}

@misc{yu_rethinking_2021,
	title = {Rethinking {Token}-{Mixing} {MLP} for {MLP}-based {Vision} {Backbone}},
	url = {http://arxiv.org/abs/2106.14882},
	doi = {10.48550/arXiv.2106.14882},
	abstract = {In the past decade, we have witnessed rapid progress in the machine vision backbone. By introducing the inductive bias from the image processing, convolution neural network (CNN) has achieved excellent performance in numerous computer vision tasks and has been established as {\textbackslash}emph\{de facto\} backbone. In recent years, inspired by the great success achieved by Transformer in NLP tasks, vision Transformer models emerge. Using much less inductive bias, they have achieved promising performance in computer vision tasks compared with their CNN counterparts. More recently, researchers investigate using the pure-MLP architecture to build the vision backbone to further reduce the inductive bias, achieving good performance. The pure-MLP backbone is built upon channel-mixing MLPs to fuse the channels and token-mixing MLPs for communications between patches. In this paper, we re-think the design of the token-mixing MLP. We discover that token-mixing MLPs in existing MLP-based backbones are spatial-specific, and thus it is sensitive to spatial translation. Meanwhile, the channel-agnostic property of the existing token-mixing MLPs limits their capability in mixing tokens. To overcome those limitations, we propose an improved structure termed as Circulant Channel-Specific (CCS) token-mixing MLP, which is spatial-invariant and channel-specific. It takes fewer parameters but achieves higher classification accuracy on ImageNet1K benchmark.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Yu, Tan and Li, Xu and Cai, Yunfeng and Sun, Mingming and Li, Ping},
	month = jun,
	year = {2021},
	note = {arXiv:2106.14882 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/29AMHKIC/Yu et al. - 2021 - Rethinking Token-Mixing MLP for MLP-based Vision B.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/MRMVVQVX/2106.html:text/html},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	annote = {Comment: Tech report},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/QDVYBBGS/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/B2S2N94N/1512.html:text/html},
}

@misc{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/UN6I5AKH/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/4Y2HI943/2103.html:text/html},
}

@misc{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/KAQHJ3GU/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/Z357WKD2/1409.html:text/html},
}

@misc{tan_efficientnet_2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	doi = {10.48550/arXiv.1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Machine Learning, notion},
	annote = {Comment: ICML 2019},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/R39MURHQ/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/929EE43I/1905.html:text/html},
}

@article{huang_convolutionally_2024,
	series = {{ACM} {Other} conferences},
	title = {Convolutionally {Enhanced} {Feature} {Fusion} {Visual} {Transformer} for {Fine}-{Grained} {Visual} {Classification}},
	issn = {9798400709234},
	url = {https://acm.tec.elogim.com/doi/10.1145/3651671.3651752},
	doi = {10.1145/3651671.3651752},
	abstract = {Fine-grained image classification is a popular research topic in computer vision and pattern recognition, where the goal is to recognize and classify subclasses of objects in images at the fine-grained level. In recent years, Transformer's self-attention mechanism has been increasingly introduced into fine-grained image classification tasks due to its ability to naturally focus on the most discriminative regions of the object. In this paper, a new Convolutionally Enhanced Feature Fusion Visual Transformer method is proposed based on the Feature Fusion Visual Transformer by introducing convolutional operations. Firstly, for the original input image, patches are not directly labeled, but extracted from the generated low-level features; Secondly, the computational complexity at the multi-head attention layer is reduced through spatial-reduction attention, which also reduces memory consumption; Finally, the inverted residual feed-forward network is applied to each encoder to improve the network's expression ability. Comparative experiments on four datasets show that the method improves the accuracy of fine-grained image feature extraction and reduces the computation and memory consumption by improving the self-attention layer to improve the efficiency and performance of the model.},
	urldate = {2025-02-04},
	journal = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing},
	author = {Huang, Min and {https://orcid.org/0000-0003-2744-0455} and {View Profile} and Zhu, Saixing and {https://orcid.org/0000-0003-0491-9893} and {View Profile} and Wang, Zehua and {https://orcid.org/0000-0002-5399-362X} and {View Profile} and Qu, Shuanghong and {https://orcid.org/0000-0002-8270-9747} and {View Profile}},
	month = feb,
	year = {2024},
	keywords = {notion, Convolutional neural network, Fine-grained images, Spatial-reduction attention, Visual transformer},
	pages = {447--452},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/HHZAF45D/Huang et al. - 2024 - Convolutionally Enhanced Feature Fusion Visual Tra.pdf:application/pdf},
}

@article{russell_circumplex_1980,
	title = {A circumplex model of affect.},
	volume = {39},
	issn = {1939-1315, 0022-3514},
	url = {https://doi.apa.org/doi/10.1037/h0077714},
	doi = {10.1037/h0077714},
	language = {en},
	number = {6},
	urldate = {2025-02-12},
	journal = {Journal of Personality and Social Psychology},
	author = {Russell, James A.},
	month = dec,
	year = {1980},
	keywords = {notion},
	pages = {1161--1178},
	file = {Russell - 1980 - A circumplex model of affect..pdf:/Users/aaronmac/Zotero/storage/N3S8JDC4/Russell - 1980 - A circumplex model of affect..pdf:application/pdf},
}

@misc{vats_facial_2023,
	title = {Facial {Expression} {Recognition} using {Squeeze} and {Excitation}-powered {Swin} {Transformers}},
	url = {http://arxiv.org/abs/2301.10906},
	doi = {10.48550/arXiv.2301.10906},
	abstract = {The ability to recognize and interpret facial emotions is a critical component of human communication, as it allows individuals to understand and respond to emotions conveyed through facial expressions and vocal tones. The recognition of facial emotions is a complex cognitive process that involves the integration of visual and auditory information, as well as prior knowledge and social cues. It plays a crucial role in social interaction, affective processing, and empathy, and is an important aspect of many realworld applications, including human-computer interaction, virtual assistants, and mental health diagnosis and treatment. The development of accurate and efficient models for facial emotion recognition is therefore of great importance and has the potential to have a significant impact on various fields of study.The field of Facial Emotion Recognition (FER) is of great significance in the areas of computer vision and artificial intelligence, with vast commercial and academic potential in fields such as security, advertising, and entertainment. We propose a FER framework that employs Swin Vision Transformers (SwinT) and squeeze and excitation block (SE) to address vision tasks. The approach uses a transformer model with an attention mechanism, SE, and SAM to improve the efficiency of the model, as transformers often require a large amount of data. Our focus was to create an efficient FER model based on SwinT architecture that can recognize facial emotions using minimal data. We trained our model on a hybrid dataset and evaluated its performance on the AffectNet dataset, achieving an F1-score of 0.5420, which surpassed the winner of the Affective Behavior Analysis in the Wild (ABAW) Competition held at the European Conference on Computer Vision (ECCV) 2022 [10].},
	language = {en},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Vats, Arpita and Chadha, Aman},
	month = apr,
	year = {2023},
	note = {arXiv:2301.10906 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	annote = {Comment: arXiv admin note: text overlap with arXiv:2103.14030 by other authors},
	file = {Vats and Chadha - 2023 - Facial Expression Recognition using Squeeze and Ex.pdf:/Users/aaronmac/Zotero/storage/PAMBQJ28/Vats and Chadha - 2023 - Facial Expression Recognition using Squeeze and Ex.pdf:application/pdf},
}

@article{ma_facial_2023-1,
	title = {Facial {Expression} {Recognition} with {Visual} {Transformers} and {Attentional} {Selective} {Fusion}},
	volume = {14},
	issn = {1949-3045, 2371-9850},
	url = {http://arxiv.org/abs/2103.16854},
	doi = {10.1109/TAFFC.2021.3122146},
	abstract = {Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues deﬁnitely increase the difﬁculty of FER on account of these information-deﬁcient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging two kinds of feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with the global-local attention. The fused feature maps are then ﬂattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14\%, FERPlus with 88.81\% and AffectNet with 61.85\%. The cross-dataset evaluation on CK+ shows the promising generalization capability of the proposed method.},
	language = {en},
	number = {2},
	urldate = {2025-02-12},
	journal = {IEEE Transactions on Affective Computing},
	author = {Ma, Fuyan and Sun, Bin and Li, Shutao},
	month = apr,
	year = {2023},
	note = {arXiv:2103.16854 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {1236--1248},
	file = {Ma et al. - 2023 - Facial Expression Recognition with Visual Transfor.pdf:/Users/aaronmac/Zotero/storage/PVYZVHBQ/Ma et al. - 2023 - Facial Expression Recognition with Visual Transfor.pdf:application/pdf},
}

@incollection{phon-amnuaisuk_facial_2017,
	address = {Cham},
	title = {Facial {Expression} {Recognition} {Using} a {Hybrid} {CNN}–{SIFT} {Aggregator}},
	volume = {10607},
	isbn = {978-3-319-69455-9 978-3-319-69456-6},
	url = {http://link.springer.com/10.1007/978-3-319-69456-6_12},
	abstract = {Deriving an effective facial expression recognition component is important for a successful human-computer interaction system. Nonetheless, recognizing facial expression remains a challenging task. This paper describes a novel approach towards facial expression recognition task. The proposed method is motivated by the success of Convolutional Neural Networks (CNN) on the face recognition problem. Unlike other works, we focus on achieving good accuracy while requiring only a small sample data for training. Scale Invariant Feature Transform (SIFT) features are used to increase the performance on small data as SIFT does not require extensive training data to generate useful features. In this paper, both Dense SIFT and regular SIFT are studied and compared when merged with CNN features. Moreover, an aggregator of the models is developed. The proposed approach is tested on the FER-2013 and CK+ datasets. Results demonstrate the superiority of CNN with Dense SIFT over conventional CNN and CNN with SIFT. The accuracy even increased when all the models are aggregated which generates state-of-art results on FER-2013 and CK+ datasets, where it achieved 73.4\% on FER-2013 and 99.1\% on CK+.},
	language = {en},
	urldate = {2025-02-12},
	booktitle = {Multi-disciplinary {Trends} in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Connie, Tee and Al-Shabi, Mundher and Cheah, Wooi Ping and Goh, Michael},
	editor = {Phon-Amnuaisuk, Somnuk and Ang, Swee-Peng and Lee, Soo-Young},
	year = {2017},
	doi = {10.1007/978-3-319-69456-6_12},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {notion},
	pages = {139--149},
	file = {Connie et al. - 2017 - Facial Expression Recognition Using a Hybrid CNN–S.pdf:/Users/aaronmac/Zotero/storage/ZMED2EVH/Connie et al. - 2017 - Facial Expression Recognition Using a Hybrid CNN–S.pdf:application/pdf},
}

@misc{park_what_2023,
	title = {What {Do} {Self}-{Supervised} {Vision} {Transformers} {Learn}?},
	url = {http://arxiv.org/abs/2305.00729},
	doi = {10.48550/arXiv.2305.00729},
	abstract = {We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role in the later layers, while MIM mainly focuses on the early layers. Upon these analyses, we find that CL and MIM can complement each other and observe that even the simplest harmonization can help leverage the advantages of both methods. The code is available at https://github.com/naver-ai/cl-vs-mim.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo},
	month = may,
	year = {2023},
	note = {arXiv:2305.00729 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
	annote = {Comment: ICLR 2023},
	file = {Preprint PDF:/Users/aaronmac/Zotero/storage/H2ZUJ476/Park et al. - 2023 - What Do Self-Supervised Vision Transformers Learn.pdf:application/pdf;Snapshot:/Users/aaronmac/Zotero/storage/X6YPNGFK/2305.html:text/html},
}

@misc{kim_facial_2022,
	title = {Facial {Expression} {Recognition} with {Swin} {Transformer}},
	url = {http://arxiv.org/abs/2203.13472},
	doi = {10.48550/arXiv.2203.13472},
	abstract = {The task of recognizing human facial expressions plays a vital role in various human-related systems, including health care and medical ﬁelds. With the recent success of deep learning and the accessibility of a large amount of annotated data, facial expression recognition research has been mature enough to be utilized in real-world scenarios with audio-visual datasets. In this paper, we introduce Swin transformer-based facial expression approach for an in-the-wild audio-visual dataset of the Aff-Wild2 Expression dataset. Speciﬁcally, we employ a three-stream network (i.e., Visual stream, Temporal stream, and Audio stream) for the audio-visual videos to fuse the multi-modal information into facial expression recognition. Experimental results on the Aff-Wild2 dataset show the effectiveness of our proposed multi-modal approaches.},
	language = {en},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Kim, Jun-Hwa and Kim, Namho and Won, Chee Sun},
	month = mar,
	year = {2022},
	note = {arXiv:2203.13472 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, notion},
	file = {Kim et al. - 2022 - Facial Expression Recognition with Swin Transforme.pdf:/Users/aaronmac/Zotero/storage/DMVGQAL3/Kim et al. - 2022 - Facial Expression Recognition with Swin Transforme.pdf:application/pdf},
}
