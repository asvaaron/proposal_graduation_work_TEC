\iftoggle{paper}
{
Facial Expression Recognition (FER) is a critical area of affective computing, aiming
to automatically identify emotions from facial images. With the advent of deep
learning, transformer-based models have demonstrated remarkable performance in
FER by capturing global dependencies in facial features. This research focuses on
enhancing the performance of POSTER: A Pyramid Cross-Fusion Transformer
Network for Facial Expression Recognition, a state-of-the-art visual transformer for
FER. Specifically, modifications to the MLP head are investigated, incorporating
dropout strategies, layer normalization, and adaptive activation functions to improve
robustness and generalization. These modifications are evaluated across benchmark
datasets such as RAF-DB to assess their impact on classification accuracy and model
reliability.

 
}%

