\chapter{Design}
\epigraph{``Another quote.''}{\vspace{10pt}Another Author}
\label{chapter:design}
\newpage

This chapter presents the developed algorithm/mechanism used to test the hypothesis. The contents of this section vary depending on the research effort
\section{Design criteria used (if needed)}

\subsection{Detailed specific criteria/tool}

\section{Description of the implementation of other algorithms (by other authors) used to test the hypothesis}

Section \ref{section:hypothesis} outlines the hypothesis, which proposes evaluating the model using the baseline head architecture introduced in Zheng et al. \cite{zheng_poster_2022}.

\begin{lstlisting}[language=Python, caption={Base classification head}]

class ClassificationHead(nn.Module):
    def __init__(self, input_dim: int, target_dim: int):
        super().__init__()
        self.linear = torch.nn.Linear(input_dim, target_dim)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        y_hat = self.linear(x)
        return y_hat

\end{lstlisting}

The baseline head takes the 512-dimensional feature vector produced by the backbone and directly maps it to the 7 emotion classes using a single linear layer. Because it lacks intermediate processing, normalization, or regularization, this configuration can introduce several limitations in terms of generalization, feature refinement, and robustness.For example: 



\begin{itemize}
    \item \textbf{Easy Overfitting:} 
    Without dropout, normalization, or nonlinear activations, the linear layer can memorize training patterns rather than learning general representations. 

    \item \textbf{No Feature Refinement:} 
    A single linear transformation cannot abstract or transform the 512-dimensional features \cite{steiner_how_2022}. If some features produced by the backbone are noisy, redundant, or weakly informative, the head provides no mechanism to adjust or denoise them.

    \item \textbf{Lack of Non-Linearity:} 
Since the head is purely linear, it cannot model complex relationships between features. This limits the expressiveness of the classifier, especially for subtle or overlapping facial expressions.

    \item \textbf{Sensitivity to Noisy Features:} 
The absence of normalization (e.g., BatchNorm) makes the classifier sensitive to variations in feature scale \cite{steiner_how_2022}.Any instability in the backbone's feature outputs directly affects classification.

    \item \textbf{No Regularization:} 
Without dropout or other regularization strategies, the head may rely too heavily on individual features, amplifying noise and making the decision boundary less stable.
\end{itemize}



\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=4cm,
    every node/.style={rectangle, rounded corners, draw, align=center, minimum width=3cm, minimum height=1.2cm},
    arrow/.style={->, thick}
]

\node (feat) {512 features};
\node (linear) [right of=feat] {Linear Layer};
\node (scores) [right of=linear] {7 class scores};

\draw[arrow] (feat) -- (linear);
\draw[arrow] (linear) -- (scores);

\end{tikzpicture}
\caption{MLP Base Head Output Layer Diagram}
\end{figure}








\section{Proposed algorithm}

\subsection{Design of the proposed algorithm}
%The  specific description of the design algorithm goes in this place, as well as a flow diagram with the description of how it works as seen on \textbf{Figure \ref{fig:proposedalgorithm} }
%
%\begin{figure}
%	\begin{center}
%		\includegraphics[width=1\columnwidth]{../img/image.png}
%		\caption{Example diagram.}
%		\label{fig:proposedalgorithm}
%	\end{center}
%\end{figure} 


\begin{lstlisting}[language=Python, caption={Enhanced classification head}]

class EnhancedHead_v1(nn.Module):
    """
    Enhanced head with BatchNorm + Dropout + Hidden Layer
    """

    def __init__(self, input_dim: int, target_dim: int, dropout_rate=0.3, hidden_ratio=0.5):
        super().__init__()
        hidden_dim = int(input_dim * hidden_ratio)

        self.features = nn.Sequential(
            nn.BatchNorm1d(input_dim),
            nn.Dropout(dropout_rate),
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout_rate / 2),
            nn.Linear(hidden_dim, target_dim)
        )

    def forward(self, x):
        x = x.view(x.size(0), -1)
        return self.features(x)

\end{lstlisting}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.7cm,
    every node/.style={rectangle, rounded corners, draw, align=center, minimum width=4.5cm, minimum height=1cm},
    arrow/.style={->, thick}
]

\node (in) {512 features};
\node (bn1) [below of=in] {BatchNorm (standardize)};
\node (do1) [below of=bn1] {Dropout 30\% (regularize)};
\node (lin1) [below of=do1] {Linear 512→256 (compress)};
\node (relu) [below of=lin1] {ReLU (non-linearity)};
\node (bn2) [below of=relu] {BatchNorm (stabilize)};
\node (do2) [below of=bn2] {Dropout 15\% (regularize)};
\node (lin2) [below of=do2] {Linear 256→7 (classify)};
\node (out) [below of=lin2] {7 class scores};

\draw[arrow] (in) -- (bn1);
\draw[arrow] (bn1) -- (do1);
\draw[arrow] (do1) -- (lin1);
\draw[arrow] (lin1) -- (relu);
\draw[arrow] (relu) -- (bn2);
\draw[arrow] (bn2) -- (do2);
\draw[arrow] (do2) -- (lin2);
\draw[arrow] (lin2) -- (out);

\end{tikzpicture}
\caption{MLP Enhanced Head Output Layer Diagram}
\end{figure}


\subsection{Limitations and Requirements}
A brief description of the design mechanism's limitations and requirements in accordance with the definitions in Scope and Limitations.


\subsection{Implementation}

A brief description of the implementation and pointer to git where it can be downloaded and tested. \url{https://myalgorithm.com}.
