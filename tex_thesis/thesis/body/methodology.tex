


\chapter{Methodology}
\label{chapter:methodology}
\epigraph{``It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong.''}{\vspace{10pt}Richard Feynman }

\newpage

This section presents a brief introduction to kind of statistical tool used in the experiment and the details of the experiment of the research. 
\section{Experiment Design}
\label{section:experiment-design}

The experiment will consist of 6 unique configurations derived from the combination of  two factors. Factor  $A$ corresponds to original model vs changes made in the head. Factor $B$ corresponds to the model size large/medium/small. In  \cite{zheng_poster_2022} the feature pyramid structure has features with embedding dim $DH = 512$, $DM = 256$, and $DL = 128$, respectively. For factor $A, s_a =2$ and Factor $B, s_b=3$, where $s$ is number of levels on each factor, we have a two-way factorial design $s_a \times s_b = 2 \times 3 = 6$. Each combination (cell) is a $\text{Head} \times \text{Size}$ pair (6 cells total).


For each configuration, the POSTER Visual Transformer will be trained using a facial expression dataset RAF-DB, and the performance will be evaluated based on a chosen metric, such as classification accuracy \ref{sub:accurracy} and F1-score \ref{sub:f1-score}.

The models will be trained in a controlled environment, where the only variations between runs are the levels of the factors being tested. This will allow for a rigorous comparison of how  model size, learning rate and head modifications influence the transformer’s performance in FER tasks using \gls{in-the-wild} datasets.


\section{Factors and Levels (if needed):}

\section{Measures and Combinations of the Experiment (if needed)}
The example \textbf{Table \ref{tab:factors_and_levels}} summarizes the factors and levels used in the experiments.

\begin{table}[H]
\centering
\caption{Factors and Levels}
\label{tab:factors_and_levels}
\begin{tabular}{clll}
\hline
\multicolumn{1}{l}{}             & \multicolumn{3}{c}{\textbf{Factors}}                       \\ 
\multicolumn{1}{l}{}             & \textbf{Factor 1} & \textbf{Factor 2} &  \\ \hline
\multirow{3}{*}{\textbf{Levels}} & Model Head        & Model Size        &  \\ 
                                 & Without Changes   & Tiny              &  \\ 
                                 & With Changes      & Small              &  \\ 
                                 & --                & Large              &  \\ \hline
\end{tabular}
\end{table}


%\begin{table}[H]
%\centering
%\caption{Factors and Levels}
%\label{tab:factors_and_levels}
%\begin{tabular}{clll}
%\hline
%\multicolumn{1}{l}{}             & \multicolumn{3}{c}{\textbf{Factors}}                       \\
%\multicolumn{1}{l}{}             & \textbf{Factor 1}  & \textbf{Factor 2} & \textbf{Factor 3} \\ \hline
%\multirow{3}{*}{\textbf{Levels}} & Pretrained Weights & Model Size        & MLP Head          \\
%                                 & Yes                & Base              & No Changes        \\
%                                 & No                 & Large             & \textbf{Modified MLP}      \\ \hline
%\end{tabular}
%\end{table}


%\begin{table}[h]
%	\caption{Example table.}
%	\label{tab:table}
%	\begin{center}
%		\begin{tabular}{|l|l|l|l|l|}
%			\cline{2-5}
%			\multicolumn{1}{c|}{}	& \multicolumn{4}{c|}{Factor} \\ 
%			\cline{2-5}
%			\multicolumn{1}{c|}{}	& Factor 1 & Factor 2 & Factor 3 & Factor 4 \\ \hline
%			Levels
%			& Level 1 & Level 1	& Level 1  & Level 1   \\
%			& Level 2 & Level 2  & Level 2 & Level 2 \\
%			& Level 3 & Level 3 & Level 3 & Level 3 \\
%			& Level 4 &    & Level 4  &   \\ 
%			&  &    & Level 5  &   \\ 
%			&  &    & Level 6  &     \\\hline
%		\end{tabular}
%	\end{center}
%\end{table}



\section{Response Variable}
Description of the response variable used in the research.

\subsection{Accuracy} \label{sub:accurracy}


The standard or top-1 accuracy will be used as the main metric for reviewing performance and robustness comparison between models. The metric corresponds to a simple calculation taking the correctly classified samples and dividing it by the total number of samples. For multi-class classification, Accuracy represents the ratio of correctly predicted samples to the total number of samples across all emotion classes


\begin{equation}
\label{eq:accurracy}
\text { Accuracy }=\frac{\sum_{i=1}^C T P_i}{N}
\end{equation}

Where:

\begin{itemize}
\item $C$: total number of classes(emotions)
\item $TP_i$: number of correctly predicted samples in class $i$
\item $N$: total number of samples in the dataset
\end{itemize}

\subsection{F1-Score(Macro-Averaged for Multi-class)} \label{sub:f1-score}

It's F1-score computation for each emotion independently, then averages them by giving equal weight to all classes. Macro F1-score provides a balanced metric by averaging the F1-score computed independently for each emotion class, ensuring equal importance is given to both majority and minority classes.


\begin{equation}
\label{eq:f1-macro-avg}
\text{F1}_{\text{macro}} = \frac{1}{C} \sum_{i=1}^{C} \text{F1}_i
\end{equation}

where the F1-score for each class $i$ is given by:

\begin{equation}
\text{F1}_i = 2 \times \frac{\text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}
\end{equation}

and Precision and Recall for each class $i$ are defined as:

\begin{equation}
\text{Precision}_i = \frac{TP_i}{TP_i + FP_i}
\end{equation}

\begin{equation}
\text{Recall}_i = \frac{TP_i}{TP_i + FN_i}
\end{equation}

where:
\begin{itemize}
    \item $TP_i$ (True Positives) — number of correctly predicted samples of class $i$.
    \item $FP_i$ (False Positives) — number of samples incorrectly predicted as class $i$.
    \item $FN_i$ (False Negatives) — number of samples from class $i$ incorrectly predicted as other classes.
\end{itemize}

A higher value of \ref{eq:f1-macro-avg} indicates better overall balance between precision and recall across all emotion categories.

\section{Description of the set up of the experiments performed}

The experimental setup was designed to ensure reproducibility, fairness, and controlled comparison across all evaluated configurations. All models were trained using the same data splits, preprocessing pipeline, optimization strategy, and training schedule, with the only variations being the classification head architecture and the feature pyramid size of the POSTER model. This includes the three model sizes shown in Table \ref{tab:factors_and_levels}.

Training was performed using NVIDIA GPUs with CUDA acceleration. Both local and cloud-based environments were used, including a desktop workstation equipped with two NVIDIA GeForce RTX 3060 GPUs and Google Colab Pro Plus instances with dynamically allocated NVIDIA GPUs. All experiments were executed using PyTorch, and deterministic CUDA behavior was enforced by fixing random seeds for Python, NumPy, and PyTorch.

Each model configuration was trained for 250 epochs under identical hyperparameter settings, including a training batch size of 120, a validation batch size of 30, the Adam optimizer, and a learning rate of $4 \times 10^{-4}$. Early stopping was not applied in order to ensure consistent training dynamics across all experimental runs. Model checkpoints and training logs were stored for post hoc analysis and metric aggregation.



\section{Description of the inputs of the system}


The input to the system consists of static facial images obtained from the Real World Affective Faces Database (RAF-DB). All images are processed using a standardized preprocessing pipeline to ensure consistency and reproducibility across all the experiments. Each image is converted to RGB format and resized to a fixed spatial resolution of $224 \times 224$ pixels, which is compatible with the POSTER backbone and Vision Transformer based architectures. Pixel intensities are normalized using channel wise mean and standard deviation values computed from the training dataset. After preprocessing, each image is represented as a tensor with three channels and a spatial resolution of 224 by 224 pixels $(3 \times 224 \times 244)$ and is passed to the feature extraction stage of the model. 


\begin{figure}[H]
\centering
   \includegraphics[width=0.90\textwidth]{../images/poster_architecture_pretrained.png}
\caption{POSTER Cross-Fusion Transformer Architecture showing Pretrained models}
\label{fig:poster_cross_function_architecture_pretrained}
\end{figure}

The architecture uses a dual branch design in which two complementary feature representations are extracted in parallel. Both branches rely on pretrained models. One branch focuses on facial landmark based features and is implemented using MobileFaceNet\footnote{\url{https://github.com/cunjian/pytorch_face_landmark}}. This branch captures geometric deformations related to facial muscle movements such as smiles, frowns, and eyebrow raises. The second branch relies on an IR50 based appearance encoder, which captures fine grained texture and intensity variations such as wrinkles, shading, and facial tension. This dual branch strategy is particularly well suited for RAF-DB, as in the wild facial expressions exhibit both geometric and appearance based variability. A detailed explanation of this architecture is shown in figure \ref{fig:poster_cross_function_architecture_pretrained}.


The mixed feature representation produced by the dual branch architecture serves as the direct input to the modified classification head evaluated in this work. After multi-scale pyramid fusion, the features are refined using a channel wise attention mechanism implemented through a Squeeze and Excitation block. This operation emphasizes informative feature channels while suppressing less relevant responses, resulting in a compact and discriminative representation. The output of this attention enhanced feature vector is then passed to the proposed classification head, where architectural variations are introduced and evaluated. By keeping the input feature representation fixed and modifying only the structure of the classification head, this methodology enables a controlled analysis of the impact of head design on facial expression recognition performance using RAF-DB.


\section{Data Recollection}

%Description of the data recollection process. Usually, it is a good idea to present a pointer to a GitHub page where all data and scripts used are available.


For the experiment the  Real-world Affective Faces Database (RAF-DB) \footnote{https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset} \footnote{http://www.whdeng.cn/RAF/model1.html} will be used. This is a widely used dataset in the field of Facial Expression Recognition (FER).

The images, collected from the Internet, are representative of diverse real-world conditions, including variations in ethnicity, age, gender, lighting, and occlusions (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc. These particular features make RAF-DB a challenging dataset, ideal for evaluating FER models under realistic settings.

Images in RAF-DB were labeled by 40 human annotators through crowdsourcing, and a majority voting scheme was used to finalize the annotations. The FER dataset flavor used in this thesis work includes 12,271 images for training and 3,068 for testing, focusing primarily on static image-based FER. Its real-world complexity, such as subtle expressions, extreme head poses, and high intra-class variability has driven advancements in FER models. 

RAF-DB has been instrumental in benchmarking state-of-the-art models like ResNet-50 \cite{he_deep_2015}, Swin Transformer \cite{liu_swin_2021}, and POSTER \cite{zheng_poster_2022}, which achieved accuracies of 86.34 \%, 90.97 \%, and 92\%+, respectively.



\section{Statistical Tool Used}

Include a section briefly explaining the statistical tool used to analyze the data from the experiments.
\subsection{Hypothesis of the statistical tool used}

\textbf{Null hypothesis:}

\textbf{Alternate hypothesis:} 
