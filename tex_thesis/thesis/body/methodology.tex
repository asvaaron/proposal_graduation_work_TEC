


\chapter{Methodology}
\label{chapter:methodology}
\epigraph{``It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong.''}{\vspace{10pt}Richard Feynman}

\newpage

This section presents a brief introduction to the statistical tools used in the experiment, as well as a detailed description of the experiment used in the research. 


\section{Experiment Design}
\label{section:experiment-design}

The experiment consisted of 6 unique configurations derived from the combination of  two factors. Factor  $A$ corresponds to original model vs changes made in the head. Factor $B$ corresponds to the model size large/medium/small. In  \cite{zheng_poster_2022} the feature pyramid structure has features with embedding dim $DH = 512$, $DM = 256$, and $DL = 128$, respectively. For factor $A, s_a =2$ and Factor $B, s_b=3$, where $s$ is number of levels on each factor, we have a two-way factorial design $s_a \times s_b = 2 \times 3 = 6$. Each combination (cell) is a $\text{Head} \times \text{Size}$ pair (6 cells total).


For each configuration, the POSTER Visual Transformer was trained using a facial expression dataset RAF-DB, and the performance was evaluated based on a chosen metric, such as classification accuracy Section \ref{sub:accuracy} and F1-score Section \ref{sub:f1-score}.

The models were trained in a controlled environment, where the only variations between runs are the levels of the factors being tested. This allowed for a rigorous comparison of how  model size, learning rate and head modifications influence the transformer's performance in FER tasks using \gls{in-the-wild} datasets.


%\section{Factors and Levels (if needed):}

\section{Factors and Levels}

%\section{Measures and Combinations of the Experiment (if needed)}

\section{Measures and Combinations of the Experiment}

Table \ref{tab:factors_and_levels} summarizes the factors and levels used on all of the experiments. 


\begin{table}[H]
\centering
\caption{Experimental Factors and Levels}
\label{tab:factors_and_levels}
\begin{tabular}{lll}
\toprule
\textbf{Factor} & \textbf{Description} & \textbf{Levels} \\
\midrule
Factor 1 & Model Head Architecture 
         & Without Changes \\
         &                       
         & With Changes \\
\midrule
Factor 2 & Model Size             
         & Tiny \\
         &                       
         & Small \\
         &                       
         & Large \\
\bottomrule
\end{tabular}
\end{table}



%\begin{table}[H]
%\centering
%\caption{Factors and Levels}
%\label{tab:factors_and_levels}
%\begin{tabular}{clll}
%\hline
%\multicolumn{1}{l}{}             & \multicolumn{3}{c}{\textbf{Factors}}                       \\
%\multicolumn{1}{l}{}             & \textbf{Factor 1}  & \textbf{Factor 2} & \textbf{Factor 3} \\ \hline
%\multirow{3}{*}{\textbf{Levels}} & Pretrained Weights & Model Size        & MLP Head          \\
%                                 & Yes                & Base              & No Changes        \\
%                                 & No                 & Large             & \textbf{Modified MLP}      \\ \hline
%\end{tabular}
%\end{table}


%\begin{table}[h]
%	\caption{Example table.}
%	\label{tab:table}
%	\begin{center}
%		\begin{tabular}{|l|l|l|l|l|}
%			\cline{2-5}
%			\multicolumn{1}{c|}{}	& \multicolumn{4}{c|}{Factor} \\ 
%			\cline{2-5}
%			\multicolumn{1}{c|}{}	& Factor 1 & Factor 2 & Factor 3 & Factor 4 \\ \hline
%			Levels
%			& Level 1 & Level 1	& Level 1  & Level 1   \\
%			& Level 2 & Level 2  & Level 2 & Level 2 \\
%			& Level 3 & Level 3 & Level 3 & Level 3 \\
%			& Level 4 &    & Level 4  &   \\ 
%			&  &    & Level 5  &   \\ 
%			&  &    & Level 6  &     \\\hline
%		\end{tabular}
%	\end{center}
%\end{table}



\section{Response Variable}
This sections shows a description of all the response variables used and obtained during the development of this research.

For the next sections the following concepts are defined: 

\begin{itemize}
    \item $TP_i$ (True Positives) — number of correctly predicted samples of class $i$.
    \item $FP_i$ (False Positives) — number of samples incorrectly predicted as class $i$.
    \item $FN_i$ (False Negatives) — number of samples from class $i$ incorrectly predicted as other classes.
\end{itemize}

\subsection{Accuracy} \label{sub:accurracy}


The standard or top-1 accuracy was used as the main metric for reviewing performance and robustness comparison between models. The metric corresponds to a simple calculation taking the correctly classified samples and dividing it by the total number of samples. Equation \ref{eq:accuracy} shows how accuracy is calculated. 

For multi-class classification, Accuracy represents the ratio of correctly predicted samples to the total number of samples across all emotion classes. Additionally, both training and validation accuracy were monitored throughout the learning process to assess the model's generalization capability and detect potential overfitting, an example of the curves comparison is shown on image \ref{fig:training-validation-acurracy-example}.

\begin{figure}[H]
\centering
   \includegraphics[width=0.90\textwidth]{../images/training-validation-acurracy-example.png}
\caption{Training and validation accuracy per epoch example \\ \textbf{Source:} Facial Expression Recognition using Squeeze and Excitation-powered Swin  Transformers\cite{vats_facial_2023}}
\label{fig:training-validation-acurracy-example}
\end{figure}



\begin{equation}
\label{eq:accurracy}
\text { Accuracy }=\frac{\sum_{i=1}^C T P_i}{N}
\end{equation}

Where:

\begin{itemize}
\item $C$: total number of classes(emotions)
\item $TP_i$: number of correctly predicted samples in class $i$
\item $N$: total number of samples in the dataset
\end{itemize}

\subsection{F1-Score(Macro-Averaged for Multi-class)} \label{sub:f1-score}

F1-score is computed for each emotion independently, then averages them by giving equal weight to all classes. At the same time macro-averaged f1-score, equation \ref{eq:f1-macro-avg}, provides a balanced metric by averaging the f1-score computed independently for each emotion class, ensuring equal importance is given to both majority and minority classes.


\begin{equation}
\label{eq:f1-macro-avg}
\text{F1}_{\text{macro}} = \frac{1}{C} \sum_{i=1}^{C} \text{F1}_i
\end{equation}

where the f1-score for each class $i$ is given by:

\begin{equation}
\text{F1}_i = 2 \times \frac{\text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}
\end{equation}

%Precision \ref{eq:precision-class} recall \ref{eq:recall-class} for each class $i$ are defined as follows:

\subsection{Precision} \label{sub:precision}

Precision is an evaluation metric that measures the accuracy of the model’s positive predictions, equation \ref{eq:precision-class}. In Facial Expression Recognition (FER), precision quantifies how many of the samples predicted as a specific emotion truly belong to that emotion class.

\begin{equation}
\label{eq:precision-class}
\text{Precision}_i = \frac{TP_i}{TP_i + FP_i}
\end{equation}

A high precision score indicates that the model produces fewer false positives, meaning it is less likely to incorrectly assign an emotion label to a facial expression that does not belong to that category. This is particularly important in FER tasks where certain emotions share visual similarities, leading to potential misclassification (e.g., happiness vs. surprise). In this research, precision is computed on a per-class basis and aggregated to evaluate the reliability of the modified visual transformer’s predictions. 


\subsection{Recall} \label{sub:recall}

Recall is a fundamental evaluation metric used to measure the model’s ability to correctly identify instances of a given class, formally the equation \ref{eq:recall-class}. In the context of Facial Expression Recognition (FER), recall quantifies how effectively the proposed model detects each facial expression among all samples that truly belong to that expression class.

\begin{equation}
\label{eq:recall-class}
\text{Recall}_i = \frac{TP_i}{TP_i + FN_i}
\end{equation}

A high recall value indicates that the model successfully minimizes false negatives, which is particularly important in FER tasks where under-detection of certain challenging emotions (e.g., fear or disgust) can negatively impact downstream applications such as affective computing or human–computer interaction


A higher value of \ref{eq:f1-macro-avg} indicates better overall balance between precision \ref{sub:precision} and recall \ref{sub:recall} across all emotion categories.

\subsection{Confusion Matrix} \label{sub:confusion-matrix}

To further analyze the classification behavior of the model, confusion matrices are employed. A confusion matrix provides a detailed breakdown of predicted versus actual class labels,\ref{fig:confusion-matrix-poster} enabling a comprehensive assessment of both correct and incorrect predictions across all emotion categories.

Each row of the confusion matrix corresponds to the true class labels, while each column represents the predicted labels. Diagonal elements indicate correct classifications, whereas off-diagonal elements highlight specific misclassification patterns between emotion classes.



\begin{figure}[H]
\centering
   \includegraphics[width=0.90\textwidth]{../images/poster_cm_datasets.png}
\caption{POSTER Confusion Matrix multiple datasets \\ \textbf{Source:} POSTER\cite{zheng_poster_2022}}
\label{fig:confusion-matrix-poster}
\end{figure}

\subsection{Loss Curves} \label{sub:loss-curves}

Loss functions play a critical role in guiding the optimization process during model training. In this work, the training loss is monitored across epochs to evaluate convergence behavior and learning stability of the proposed model variants. As shown in image \ref{fig:training-validation-loss-example} typically training and validation plots are made for each training epoch. 



\begin{figure}[H]
\centering
   \includegraphics[width=0.90\textwidth]{../images/training-validation-loss-example.png}
\caption{Training and validation loss per epoch example \\ \textbf{Source:} Facial Expression Recognition using Squeeze and Excitation-powered Swin  Transformers\cite{vats_facial_2023}}
\label{fig:training-validation-loss-example}
\end{figure}


Loss function plots are generated to visualize:

\begin{itemize}
\item Training loss evolution
\item Validation loss trends
\item Potential overfitting or under fitting behavior

\end{itemize}


A consistent decrease in training and validation loss indicates effective learning, whereas divergence between the two may suggest overfitting. By comparing loss curves across different head configurations, this study assesses how architectural modifications impact optimization efficiency and generalization performance.


\section{Description of the set up of the experiments performed}

The experimental setup was designed to ensure reproducibility, fairness, and controlled comparison across all evaluated configurations. All models were trained using the same data splits, preprocessing pipeline, optimization strategy, and training schedule, with the only variations being the classification head architecture and the feature pyramid size of the POSTER model. This includes the three model sizes shown in Table \ref{tab:factors_and_levels}.

Training was performed using NVIDIA GPUs with CUDA acceleration. Both local and cloud-based environments were used, including a desktop workstation equipped with two NVIDIA GeForce RTX 3060 GPUs and Google Colab Pro Plus instances with dynamically allocated NVIDIA GPUs. All experiments were executed using PyTorch, and deterministic CUDA behavior was enforced by fixing random seeds for Python, NumPy, and PyTorch.

Each model configuration was trained for 250 epochs under identical hyper parameter settings, including a training batch size of 120, a validation batch size of 30, the Adam optimizer, and a learning rate of $4 \times 10^{-4}$. Early stopping was not applied in order to ensure consistent training dynamics across all experimental runs. Model checkpoints and training logs were stored for post hoc analysis and metric aggregation.



\section{Description of the inputs of the system}


The input to the system consists of static facial images obtained from the Real World Affective Faces Database (RAF-DB). All images are processed using a standardized preprocessing pipeline to ensure consistency and reproducibility across all the experiments. Each image is converted to RGB format and resized to a fixed spatial resolution of $224 \times 224$ pixels, which is compatible with the POSTER backbone and Vision Transformer based architectures. Pixel intensities are normalized using channel wise mean and standard deviation values computed from the training dataset. After preprocessing, each image is represented as a tensor with three channels and a spatial resolution of 224 by 224 pixels $(3 \times 224 \times 224)$ and is passed to the feature extraction stage of the model. 


\begin{figure}[H]
\centering
   \includegraphics[width=0.90\textwidth]{../images/poster_architecture_pretrained.png}
\caption{POSTER Cross-Fusion Transformer Architecture showing Pretrained models \\ \textbf{Source:} POSTER\cite{zheng_poster_2022}}
\label{fig:poster_cross_function_architecture_pretrained}
\end{figure}

\subsection{Feature Extraction Backbones: IR50 and MobileFaceNet}


\subsubsection{IR50 Appearance Encoder}

The IR50 backbone corresponds to an InsightFace-style ResNet-50 architecture, commonly referred to as IResNet-50 \footnote{\url{https://github.com/deepinsight/insightface}}. This model has been widely used in face analysis tasks \cite{deng_arcface_2022} 
\cite{vedaldi_sub-center_2020} due to its strong representational capacity for capturing fine-grained facial appearance features. In the POSTER \cite{zheng_poster_2022} framework, the IR50 branch functions acts as an appearance encoder, extracting texture-based information such as wrinkles, shading variations, and subtle intensity changes around key facial regions.


IR-based backbones are frequently pretrained using margin-based loss functions, such as ArcFace \cite{deng_arcface_2022}, which enhance inter-class separability and intra-class compactness in the learned embedding space. Although originally developed for face identity recognition, these pretrained representations have proven effective when transferred to related facial analysis tasks, including facial expression recognition, due to their robustness to pose, illumination, and identity variation.


Within this research, the IR50 backbone is kept fixed across all experimental configurations. This design choice ensures that any observed performance differences can be attributed exclusively to modifications in the classification head rather than changes in the feature extraction stage.


\subsubsection{MobileFaceNet Geometric Encoder}

MobileFaceNet\footnote{\url{https://github.com/cunjian/pytorch_face_landmark}} \footnote{\url{https://github.com/TreB1eN/InsightFace_Pytorch?tab=readme-ov-file}} is a lightweight convolutional neural network architecture specifically designed for efficient facial representation learning. Its compact structure enables the extraction of discriminative geometric features while maintaining a low computational footprint \cite{tan_efficientnet_2020} 
\cite{chen_mobilefacenets_2018}. In POSTER \cite{zheng_poster_2022}, MobileFaceNet is employed as the landmark-oriented branch, focusing on capturing facial geometry and muscle deformation patterns associated with expressions, such as eyebrow movements, mouth opening, and cheek displacement.


The use of MobileFaceNet complements the appearance-focused IR50 branch by emphasizing structural information that remains informative even under challenging in-the-wild conditions, such as partial occlusion or low image quality. Through the pyramid cross-fusion mechanism, geometric and appearance-based features are combined at multiple scales, improving the model’s robustness to intra-class variation and inter-class similarity. As with the IR50 branch, the MobileFaceNet encoder remains unchanged throughout all experiments in this thesis, enabling a controlled evaluation of the proposed classification head modifications.

\subsection{POSTER Architecture}

The architecture uses a dual branch design in which two complementary feature representations are extracted in parallel. Both branches rely on pretrained models. The branch that focuses on facial landmark based features is implemented using MobileFaceNet. This branch captures geometric deformations related to facial muscle movements such as smiles, frowns, and eyebrow raises. The second branch relies on an IR50 based appearance encoder, which captures fine grained texture and intensity variations such as wrinkles, shading, and facial tension. This dual branch strategy is particularly well suited for RAF-DB, as in the wild facial expressions exhibit both geometric and appearance based variability. A detailed explanation of this architecture is shown in figure \ref{fig:poster_cross_function_architecture_pretrained}.


The mixed feature representation produced by the dual branch architecture serves as the direct input to the modified classification head evaluated in this work. After multi-scale pyramid fusion, the features are refined using a channel wise attention mechanism implemented through a Squeeze and Excitation block. This operation emphasizes informative feature channels while suppressing less relevant responses, resulting in a compact and discriminative representation. The output of this attention enhanced feature vector is then passed to the proposed classification head, where architectural variations are introduced and evaluated. By keeping the input feature representation fixed and modifying only the structure of the classification head, this methodology enables a controlled analysis of the impact of head design on facial expression recognition performance using RAF-DB.


\section{Data Recollection}

This section shows a brief description of the data recollection process. 

%Usually, it is a good idea to present a pointer to a GitHub page where all data and scripts used are available.

\subsection{RAF-DB}

For conducting all of the experiments the dataset Real-world Affective Faces Database (RAF-DB)\footnote{\url{https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset}}\footnote{\url{http://www.whdeng.cn/RAF/model1.html}} was be used. This corresponds to a widely used dataset in the field of Facial Expression Recognition (FER).

The images, collected from the Internet, are representative of diverse real-world conditions, including variations in ethnicity, age, gender, lighting, and occlusions (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc. These particular features make RAF-DB a challenging dataset, ideal for evaluating FER models under realistic settings.

Images in RAF-DB were labeled by 40 human annotators through crowdsourcing, and a majority voting scheme was used to finalize the annotations. The FER dataset flavor used in this thesis work includes 12,271 images for training and 3,068 for testing, focusing primarily on static image-based FER. Additional information of each class distribution for training and testing is provided in table \ref{tab:class-distribution}. 

The RAF-DB dataset contains facial images collected from a wide international population, exhibiting substantial diversity in ethnicity, age, and gender. Although detailed demographic labels such as age values or ethnicity categories are not explicitly provided, the dataset includes faces from Caucasian, Asian, African-descent, and Latin American groups, as well as a broad range of age groups extending from children, adults and older adults. This demographic variability, combined with variations in pose, illumination, occlusion, and background, contributes to the dataset’s realism and makes it a suitable benchmark for evaluating the generalization capability of facial expression recognition models.

\begin{table}[H]
\centering
\caption{Class distribution for training and test datasets}
\label{tab:class-distribution}
\begin{tabular}{llcc}
\hline
\textbf{Dataset} & \textbf{Class} & \textbf{Class ID} & \textbf{Samples} \\
\hline
\multirow{7}{*}{Test} 
 & Angry    & 1 & 329  \\
 & Disgust  & 2 & 74   \\
 & Fear     & 3 & 160  \\
 & Happy    & 4 & 1185 \\
 & Neutral  & 5 & 478  \\
 & Sad      & 6 & 162  \\
 & Surprise & 7 & 680  \\
\cline{2-4}
 & \textbf{Total} &  & \textbf{3068} \\
\hline
\multirow{7}{*}{Train} 
 & Angry    & 1 & 1290 \\
 & Disgust  & 2 & 281  \\
 & Fear     & 3 & 717  \\
 & Happy    & 4 & 4772 \\
 & Neutral  & 5 & 1982 \\
 & Sad      & 6 & 705  \\
 & Surprise & 7 & 2524 \\
\cline{2-4}
 & \textbf{Total} &  & \textbf{12271} \\
\hline
\end{tabular}
\end{table}



RAF-DB has been instrumental in multiple benchmarking state-of-the-art models like ResNet-50 \cite{he_deep_2015}, Swin Transformer \cite{liu_swin_2021}, and POSTER \cite{zheng_poster_2022}, which achieved accuracy values of 86.34 \%, 90.97 \%, and 92\%+, respectively.



\section{Statistical Tool Used}

%Include a section briefly explaining the statistical tool used to analyze the data from the experiments.

To evaluate the impact of the proposed classification head modifications, a statistical analysis was conducted using aggregated performance metrics obtained from multiple experimental runs. For each model configuration, the average values and standard deviation of accuracy, macro averaged f1-score, precision, recall. Using averaged metrics reduces the effect of stochastic variation introduced by random initialization and optimization dynamics, enabling a more stable and reliable comparison between configurations. 

In addition to aggregated metrics, training and validation loss curves were plotted for each model configuration in order to analyze convergence behavior and training stability. Furthermore, per-class precision and recall values were computed and visualized, and a confusion matrix was generated for each model to provide a detailed analysis of class-wise performance and misclassification patterns across the seven emotion categories.

%
%\subsection{Hypothesis of the statistical tool used}
%
%\textbf{Null hypothesis:}
%
%\textbf{Alternate hypothesis:} 
