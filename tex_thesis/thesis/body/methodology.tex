


\chapter{Methodology}
\label{chapter:methodology}
\epigraph{``It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong.''}{\vspace{10pt}Richard Feynman }

\newpage

This section presents a brief introduction to kind of statistical tool used in the experiment and the details of the experiment of the research. 
\section{Experiment Design}
\label{section:experiment-design}

The experiment will consist of 6 unique configurations derived from the combination of  two factors. Factor  $A$ corresponds to original model vs changes made in the head. Factor $B$ corresponds to the model size large/medium/small. In  \cite{zheng_poster_2022} the feature pyramid structure has features with embedding dim $DH = 512$, $DM = 256$, and $DL = 128$, respectively. For factor $A, s_a =2$ and Factor $B, s_b=3$, where $s$ is number of levels on each factor, we have a two-way factorial design $s_a \times s_b = 2 \times 3 = 6$. Each combination (cell) is a $\text{Head} \times \text{Size}$ pair (6 cells total).


For each configuration, the POSTER Visual Transformer will be trained using a facial expression dataset RAF-DB \ref{sub:raf-db}, and the performance will be evaluated based on a chosen metric, such as classification accuracy \ref{sub:accurracy} and F1-score \ref{sub:f1-score}.

The models will be trained in a controlled environment, where the only variations between runs are the levels of the factors being tested. This will allow for a rigorous comparison of how  model size, learning rate and head modifications influence the transformer’s performance in FER tasks using in \gls{in-the-wild} datasets.


\section{Factors and Levels (if needed):}

\section{Measures and Combinations of the Experiment (if needed)}
The example \textbf{Table \ref{tab:factors_and_levels}} summarizes the factors and levels used in the experiments.

\begin{table}[H]
\centering
\caption{Factors and Levels}
\label{tab:factors_and_levels}
\begin{tabular}{clll}
\hline
\multicolumn{1}{l}{}             & \multicolumn{3}{c}{\textbf{Factors}}                       \\ 
\multicolumn{1}{l}{}             & \textbf{Factor 1} & \textbf{Factor 2} &  \\ \hline
\multirow{3}{*}{\textbf{Levels}} & Model Head        & Model Size        &  \\ 
                                 & Without Changes   & Tiny              &  \\ 
                                 & With Changes      & Small              &  \\ 
                                 & --                & Large              &  \\ \hline
\end{tabular}
\end{table}


%\begin{table}[H]
%\centering
%\caption{Factors and Levels}
%\label{tab:factors_and_levels}
%\begin{tabular}{clll}
%\hline
%\multicolumn{1}{l}{}             & \multicolumn{3}{c}{\textbf{Factors}}                       \\
%\multicolumn{1}{l}{}             & \textbf{Factor 1}  & \textbf{Factor 2} & \textbf{Factor 3} \\ \hline
%\multirow{3}{*}{\textbf{Levels}} & Pretrained Weights & Model Size        & MLP Head          \\
%                                 & Yes                & Base              & No Changes        \\
%                                 & No                 & Large             & \textbf{Modified MLP}      \\ \hline
%\end{tabular}
%\end{table}


%\begin{table}[h]
%	\caption{Example table.}
%	\label{tab:table}
%	\begin{center}
%		\begin{tabular}{|l|l|l|l|l|}
%			\cline{2-5}
%			\multicolumn{1}{c|}{}	& \multicolumn{4}{c|}{Factor} \\ 
%			\cline{2-5}
%			\multicolumn{1}{c|}{}	& Factor 1 & Factor 2 & Factor 3 & Factor 4 \\ \hline
%			Levels
%			& Level 1 & Level 1	& Level 1  & Level 1   \\
%			& Level 2 & Level 2  & Level 2 & Level 2 \\
%			& Level 3 & Level 3 & Level 3 & Level 3 \\
%			& Level 4 &    & Level 4  &   \\ 
%			&  &    & Level 5  &   \\ 
%			&  &    & Level 6  &     \\\hline
%		\end{tabular}
%	\end{center}
%\end{table}



\section{Response Variable}
Description of the response variable used in the research.

\subsection{Acurracy}

The standard or top-1 acurracy will be used as the main metric for reviewing performance and robusteness compassion between models. The metric corresponds to a simple calculation taking the correctly classified samples and dividing it by the total number of samples. For multi-class classification, Accuracy represents the ratio of correctly predicted samples to the total number of samples across all emotion classes


\begin{equation}
\label{eq:accurracy}
\text { Accuracy }=\frac{\sum_{i=1}^C T P_i}{N}
\end{equation}

Where:

\begin{itemize}
\item $C$: total number of classes(emotions)
\item $TP_i$: number of correctly predicted samples in class $i$
\item $N$: total number of samples in the dataset
\end{itemize}

\subsection{F1-Score(Macro-Averaged for Multi-class)}

It's F1-score computation for each emotion independently, then averages them by giving equal weight to all classes.Macro F1-score provides a balanced metric by averaging the F1-score computed independently for each emotion class, ensuring equal importance is given to both majority and minority classes.


\begin{equation}
\label{eq:f1-macro-avg}
\text{F1}_{\text{macro}} = \frac{1}{C} \sum_{i=1}^{C} \text{F1}_i
\end{equation}

where the F1-score for each class $i$ is given by:

\begin{equation}
\text{F1}_i = 2 \times \frac{\text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}
\end{equation}

and Precision and Recall for each class $i$ are defined as:

\begin{equation}
\text{Precision}_i = \frac{TP_i}{TP_i + FP_i}
\end{equation}

\begin{equation}
\text{Recall}_i = \frac{TP_i}{TP_i + FN_i}
\end{equation}

where:
\begin{itemize}
    \item $TP_i$ (True Positives) — number of correctly predicted samples of class $i$.
    \item $FP_i$ (False Positives) — number of samples incorrectly predicted as class $i$.
    \item $FN_i$ (False Negatives) — number of samples from class $i$ incorrectly predicted as other classes.
\end{itemize}

A higher value of \ref{eq:f1-macro-avg} indicates better overall balance between precision and recall across all emotion categories.

\section{Description of the set up of the experiments performed}

\section{Description of the inputs of the system}

\section{Data Recollection}

%Description of the data recollection process. Usually, it is a good idea to present a pointer to a GitHub page where all data and scripts used are available.


For the experiment the  Real-world Affective Faces Database (RAF-DB) \footnote{https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset} \footnote{http://www.whdeng.cn/RAF/model1.html} will be used. This is a widely used dataset in the field of facial expression recognition (FER).

The images, collected from the Internet, are representative of diverse real-world conditions, including variations in ethnicity, age, gender, lighting, and occlusions (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc. These particular features make RAF-DB a challenging dataset, ideal for evaluating FER models under realistic settings.

Images in RAF-DB were labeled by 40 human annotators through crowdsourcing, and a majority voting scheme was used to finalize the annotations. The FER dataset flavor used in this thesis work includes 12,271 images for training and 3,068 for testing, focusing primarily on static image-based FER. Its real-world complexity, such as subtle expressions, extreme head poses, and high intra-class variability has driven advancements in FER models. 

RAF-DB has been instrumental in benchmarking state-of-the-art models like ResNet-50 \cite{he_deep_2015}, Swin Transformer \cite{liu_swin_2021}, and POSTER \cite{zheng_poster_2022}, which achieved accuracies of 86.34 \%, 90.97 \%, and 92\%+, respectively.



\section{Statistical Tool Used}

Include a section briefly explaining the statistical tool used to analyze the data from the experiments.
\subsection{Hypothesis of the statistical tool used}

\textbf{Null hypothesis:}

\textbf{Alternate hypothesis:} 
