
@article{mollahosseini_affectnet_2019,
	title = {{AffectNet}: {A} {Database} for {Facial} {Expression}, {Valence}, and {Arousal} {Computing} in the {Wild}},
	volume = {10},
	issn = {1949-3045, 2371-9850},
	shorttitle = {{AffectNet}},
	url = {http://arxiv.org/abs/1708.03985},
	doi = {10.1109/TAFFC.2017.2740923},
	abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
	number = {1},
	urldate = {2024-05-22},
	journal = {IEEE Transactions on Affective Computing},
	author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
	month = jan,
	year = {2019},
	note = {arXiv:1708.03985 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {18--31},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/9KM2DCAU/Mollahosseini et al. - 2019 - AffectNet A Database for Facial Expression, Valen.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/QPV3W95G/1708.html:text/html},
}

@misc{zhang_transformer-based_2022,
	title = {Transformer-based {Multimodal} {Information} {Fusion} for {Facial} {Expression} {Analysis}},
	url = {http://arxiv.org/abs/2203.12367},
	doi = {10.48550/arXiv.2203.12367},
	abstract = {Human affective behavior analysis has received much attention in human-computer interaction (HCI). In this paper, we introduce our submission to the CVPR 2022 Competition on Affective Behavior Analysis in-the-wild (ABAW). To fully exploit affective knowledge from multiple views, we utilize the multimodal features of spoken words, speech prosody, and facial expression, which are extracted from the video clips in the Aff-Wild2 dataset. Based on these features, we propose a unified transformer-based multimodal framework for Action Unit detection and also expression recognition. Specifically, the static vision feature is first encoded from the current frame image. At the same time, we clip its adjacent frames by a sliding window and extract three kinds of multimodal features from the sequence of images, audio, and text. Then, we introduce a transformer-based fusion module that integrates the static vision features and the dynamic multimodal features. The cross-attention module in the fusion module makes the output integrated features focus on the crucial parts that facilitate the downstream detection tasks. We also leverage some data balancing techniques, data augmentation techniques, and postprocessing methods to further improve the model performance. In the official test of ABAW3 Competition, our model ranks first in the EXPR and AU tracks. The extensive quantitative evaluations, as well as ablation studies on the Aff-Wild2 dataset, prove the effectiveness of our proposed method.},
	urldate = {2024-07-06},
	publisher = {arXiv},
	author = {Zhang, Wei and Qiu, Feng and Wang, Suzhen and Zeng, Hao and Zhang, Zhimeng and An, Rudong and Ma, Bowen and Ding, Yu},
	month = apr,
	year = {2022},
	note = {arXiv:2203.12367 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {Full Text:/Users/aaronmac/Zotero/storage/CKFJXCGC/Zhang et al. - 2022 - Transformer-based Multimodal Information Fusion fo.pdf:application/pdf},
}

@misc{saravanan_facial_2019,
	title = {Facial {Emotion} {Recognition} using {Convolutional} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1910.05602v1},
	abstract = {Facial expression recognition is a topic of great interest in most fields from artificial intelligence and gaming to marketing and healthcare. The goal of this paper is to classify images of human faces into one of seven basic emotions. A number of different models were experimented with, including decision trees and neural networks before arriving at a final Convolutional Neural Network (CNN) model. CNNs work better for image recognition tasks since they are able to capture spacial features of the inputs due to their large number of filters. The proposed model consists of six convolutional layers, two max pooling layers and two fully connected layers. Upon tuning of the various hyperparameters, this model achieved a final accuracy of 0.60.},
	language = {en},
	urldate = {2024-07-07},
	journal = {arXiv.org},
	author = {Saravanan, Akash and Perichetla, Gurudutt and Gayathri, Dr K. S.},
	month = oct,
	year = {2019},
	keywords = {notion},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/PSBK5G2E/Saravanan et al. - 2019 - Facial Emotion Recognition using Convolutional Neu.pdf:application/pdf},
}

@misc{kollias_affect_2021,
	title = {Affect {Analysis} in-the-wild: {Valence}-{Arousal}, {Expressions}, {Action} {Units} and a {Unified} {Framework}},
	shorttitle = {Affect {Analysis} in-the-wild},
	url = {http://arxiv.org/abs/2103.15792},
	doi = {10.48550/arXiv.2103.15792},
	abstract = {Affect recognition based on subjects' facial expressions has been a topic of major research in the attempt to generate machines that can understand the way subjects feel, act and react. In the past, due to the unavailability of large amounts of data captured in real-life situations, research has mainly focused on controlled environments. However, recently, social media and platforms have been widely used. Moreover, deep learning has emerged as a means to solve visual analysis and recognition problems. This paper exploits these advances and presents significant contributions for affect analysis and recognition in-the-wild. Affect analysis and recognition can be seen as a dual knowledge generation problem, involving: i) creation of new, large and rich in-the-wild databases and ii) design and training of novel deep neural architectures that are able to analyse affect over these databases and to successfully generalise their performance on other datasets. The paper focuses on large in-the-wild databases, i.e., Aff-Wild and Aff-Wild2 and presents the design of two classes of deep neural networks trained with these databases. The first class refers to uni-task affect recognition, focusing on prediction of the valence and arousal dimensional variables. The second class refers to estimation of all main behavior tasks, i.e. valence-arousal prediction; categorical emotion classification in seven basic facial expressions; facial Action Unit detection. A novel multi-task and holistic framework is presented which is able to jointly learn and effectively generalize and perform affect recognition over all existing in-the-wild databases. Large experimental studies illustrate the achieved performance improvement over the existing state-of-the-art in affect recognition.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Kollias, Dimitrios and Zafeiriou, Stefanos},
	month = mar,
	year = {2021},
	note = {arXiv:2103.15792 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/WP4C5RHI/Kollias and Zafeiriou - 2021 - Affect Analysis in-the-wild Valence-Arousal, Expr.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/UU475Q8P/2103.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/NES9QTJ7/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/8IDZPETB/2010.html:text/html},
}

@misc{her_batch_2024,
	title = {Batch {Transformer}: {Look} for {Attention} in {Batch}},
	shorttitle = {Batch {Transformer}},
	url = {http://arxiv.org/abs/2407.04218},
	doi = {10.48550/arXiv.2407.04218},
	abstract = {Facial expression recognition (FER) has received considerable attention in computer vision, with "in-the-wild" environments such as human-computer interaction. However, FER images contain uncertainties such as occlusion, low resolution, pose variation, illumination variation, and subjectivity, which includes some expressions that do not match the target label. Consequently, little information is obtained from a noisy single image and it is not trusted. This could significantly degrade the performance of the FER task. To address this issue, we propose a batch transformer (BT), which consists of the proposed class batch attention (CBA) module, to prevent overfitting in noisy data and extract trustworthy information by training on features reflected from several images in a batch, rather than information from a single image. We also propose multi-level attention (MLA) to prevent overfitting the specific features by capturing correlations between each level. In this paper, we present a batch transformer network (BTN) that combines the above proposals. Experimental results on various FER benchmark datasets show that the proposed BTN consistently outperforms the state-ofthe-art in FER datasets. Representative results demonstrate the promise of the proposed BTN for FER.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Her, Myung Beom and Jeong, Jisu and Song, Hojoon and Han, Ji-Hyeong},
	month = jul,
	year = {2024},
	note = {arXiv:2407.04218 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/23WRHVCF/Her et al. - 2024 - Batch Transformer Look for Attention in Batch.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/UH9LJEBL/2407.html:text/html},
}

@misc{ning_representation_2024,
	title = {Representation {Learning} and {Identity} {Adversarial} {Training} for {Facial} {Behavior} {Understanding}},
	url = {http://arxiv.org/abs/2407.11243},
	doi = {10.48550/arXiv.2407.11243},
	abstract = {Facial Action Unit (AU) detection has gained significant research attention as AUs contain complex expression information. In this paper, we unpack two fundamental factors in AU detection: data and subject identity regularization, respectively. Motivated by recent advances in foundation models, we highlight the importance of data and collect a diverse dataset Face9M, comprising 9 million facial images, from multiple public resources. Pretraining a masked autoencoder on Face9M yields strong performance in AU detection and facial expression tasks. We then show that subject identity in AU datasets provides a shortcut learning for the model and leads to sub-optimal solutions to AU predictions. To tackle this generic issue of AU tasks, we propose Identity Adversarial Training (IAT) and demonstrate that a strong IAT regularization is necessary to learn identity-invariant features. Furthermore, we elucidate the design space of IAT and empirically show that IAT circumvents the identity shortcut learning and results in a better solution. Our proposed methods, Facial Masked Autoencoder (FMAE) and IAT, are simple, generic and effective. Remarkably, the proposed FMAE-IAT approach achieves new state-of-the-art F1 scores on BP4D (67.1{\textbackslash}\%), BP4D+ (66.8{\textbackslash}\%), and DISFA (70.1{\textbackslash}\%) databases, significantly outperforming previous work. We release the code and model at https://github.com/forever208/FMAE-IAT, the first open-sourced facial model pretrained on 9 million diverse images.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Ning, Mang and Salah, Albert Ali and Ertugrul, Itir Onal},
	month = jul,
	year = {2024},
	note = {arXiv:2407.11243 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/B29YZRWL/Ning et al. - 2024 - Representation Learning and Identity Adversarial T.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/K5E3UAZ4/2407.html:text/html},
}

@article{valle_multi-task_2021,
	title = {Multi-task head pose estimation in-the-wild},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/2202.02299},
	doi = {10.1109/TPAMI.2020.3046323},
	abstract = {We present a deep learning-based multi-task approach for head pose estimation in images. We contribute with a network architecture and training strategy that harness the strong dependencies among face pose, alignment and visibility, to produce a top performing model for all three tasks. Our architecture is an encoder-decoder CNN with residual blocks and lateral skip connections. We show that the combination of head pose estimation and landmark-based face alignment significantly improve the performance of the former task. Further, the location of the pose task at the bottleneck layer, at the end of the encoder, and that of tasks depending on spatial information, such as visibility and alignment, in the final decoder layer, also contribute to increase the final performance. In the experiments conducted the proposed model outperforms the state-of-the-art in the face pose and visibility tasks. By including a final landmark regression step it also produces face alignment results on par with the state-of-the-art.},
	number = {8},
	urldate = {2024-08-05},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Valle, Roberto and Buenaposada, José Miguel and Baumela, Luis},
	month = aug,
	year = {2021},
	note = {arXiv:2202.02299 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {2874--2881},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/CPWBJE4M/Valle et al. - 2021 - Multi-task head pose estimation in-the-wild.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/V9SH8AE2/2202.html:text/html},
}

@article{zhang_dual-direction_2023,
	title = {A {Dual}-{Direction} {Attention} {Mixed} {Feature} {Network} for {Facial} {Expression} {Recognition}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/17/3595},
	doi = {10.3390/electronics12173595},
	abstract = {In recent years, facial expression recognition (FER) has garnered significant attention within the realm of computer vision research. This paper presents an innovative network called the Dual-Direction Attention Mixed Feature Network (DDAMFN) specifically designed for FER, boasting both robustness and lightweight characteristics. The network architecture comprises two primary components: the Mixed Feature Network (MFN) serving as the backbone, and the Dual-Direction Attention Network (DDAN) functioning as the head. To enhance the network’s capability in the MFN, resilient features are extracted by utilizing mixed-size kernels. Additionally, a new Dual-Direction Attention (DDA) head that generates attention maps in two orientations is proposed, enabling the model to capture long-range dependencies effectively. To further improve the accuracy, a novel attention loss mechanism for the DDAN is introduced with different heads focusing on distinct areas of the input. Experimental evaluations on several widely used public datasets, including AffectNet, RAF-DB, and FERPlus, demonstrate the superiority of the DDAMFN compared to other existing models, which establishes that the DDAMFN as the state-of-the-art model in the field of FER.},
	language = {en},
	number = {17},
	urldate = {2024-08-05},
	journal = {Electronics},
	author = {Zhang, Saining and Zhang, Yuhang and Zhang, Ye and Wang, Yufei and Song, Zhigang},
	month = jan,
	year = {2023},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {coordinate attention, facial expression recognition, MixConv, MobileFaceNets, notion},
	pages = {3595},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/S2MYJ339/Zhang et al. - 2023 - A Dual-Direction Attention Mixed Feature Network f.pdf:application/pdf},
}

@misc{wang_region_2019,
	title = {Region {Attention} {Networks} for {Pose} and {Occlusion} {Robust} {Facial} {Expression} {Recognition}},
	url = {http://arxiv.org/abs/1905.04075},
	doi = {10.48550/arXiv.1905.04075},
	abstract = {Occlusion and pose variations, which can change facial appearance significantly, are two major obstacles for automatic Facial Expression Recognition (FER). Though automatic FER has made substantial progresses in the past few decades, occlusion-robust and pose-invariant issues of FER have received relatively less attention, especially in real-world scenarios. This paper addresses the real-world pose and occlusion robust FER problem with three-fold contributions. First, to stimulate the research of FER under real-world occlusions and variant poses, we build several in-the-wild facial expression datasets with manual annotations for the community. Second, we propose a novel Region Attention Network (RAN), to adaptively capture the importance of facial regions for occlusion and pose variant FER. The RAN aggregates and embeds varied number of region features produced by a backbone convolutional neural network into a compact fixed-length representation. Last, inspired by the fact that facial expressions are mainly defined by facial action units, we propose a region biased loss to encourage high attention weights for the most important regions. We validate our RAN and region biased loss on both our built test datasets and four popular datasets: FERPlus, AffectNet, RAF-DB, and SFEW. Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose. Our method also achieves state-of-the-art results on FERPlus, AffectNet, RAF-DB, and SFEW. Code and the collected test data will be publicly available.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Wang, Kai and Peng, Xiaojiang and Yang, Jianfei and Meng, Debin and Qiao, Yu},
	month = sep,
	year = {2019},
	note = {arXiv:1905.04075 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/4JEPHGFB/Wang et al. - 2019 - Region Attention Networks for Pose and Occlusion R.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/V9W38XGB/1905.html:text/html},
}

@article{ma_facial_2023,
	title = {Facial {Expression} {Recognition} with {Visual} {Transformers} and {Attentional} {Selective} {Fusion}},
	volume = {14},
	issn = {1949-3045, 2371-9850},
	url = {http://arxiv.org/abs/2103.16854},
	doi = {10.1109/TAFFC.2021.3122146},
	abstract = {Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues deﬁnitely increase the difﬁculty of FER on account of these information-deﬁcient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging two kinds of feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with the global-local attention. The fused feature maps are then ﬂattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14\%, FERPlus with 88.81\% and AffectNet with 61.85\%. The cross-dataset evaluation on CK+ shows the promising generalization capability of the proposed method.},
	language = {en},
	number = {2},
	urldate = {2024-08-17},
	journal = {IEEE Transactions on Affective Computing},
	author = {Ma, Fuyan and Sun, Bin and Li, Shutao},
	month = apr,
	year = {2023},
	note = {arXiv:2103.16854 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {1236--1248},
	file = {Ma et al. - 2023 - Facial Expression Recognition with Visual Transfor.pdf:/Users/aaronmac/Zotero/storage/HSRGVYFV/Ma et al. - 2023 - Facial Expression Recognition with Visual Transfor.pdf:application/pdf},
}

@inproceedings{zhao_former-dfer_2021,
	address = {New York, NY, USA},
	series = {{MM} '21},
	title = {Former-{DFER}: {Dynamic} {Facial} {Expression} {Recognition} {Transformer}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {Former-{DFER}},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475292},
	doi = {10.1145/3474085.3475292},
	abstract = {This paper proposes a dynamic facial expression recognition transformer (Former-DFER) for the in-the-wild scenario. Specifically, the proposed Former-DFER mainly consists of a convolutional spatial transformer (CS-Former) and a temporal transformer (T-Former). The CS-Former consists of five convolution blocks and N spatial encoders, which is designed to guide the network to learn occlusion and pose-robust facial features from the spatial perspective. And the temporal transformer consists of M temporal encoders, which is designed to allow the network to learn contextual facial features from the temporal perspective. The heatmaps of the leaned facial features demonstrate that the proposed Former-DFER is capable of handling the issues such as occlusion, non-frontal pose, and head motion. And the visualization of the feature distribution shows that the proposed method can learn more discriminative facial features. Moreover, our Former-DFER also achieves state-of-the-art results on the DFEW and AFEW benchmarks.},
	urldate = {2024-08-20},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Zengqun and Liu, Qingshan},
	month = oct,
	year = {2021},
	keywords = {notion},
	pages = {1553--1561},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/ILC9RHKA/Zhao and Liu - 2021 - Former-DFER Dynamic Facial Expression Recognition.pdf:application/pdf},
}

@misc{zheng_poster_2022,
	title = {{POSTER}: {A} {Pyramid} {Cross}-{Fusion} {Transformer} {Network} for {Facial} {Expression} {Recognition}},
	shorttitle = {{POSTER}},
	url = {https://arxiv.org/abs/2204.04083v2},
	abstract = {Facial expression recognition (FER) is an important task in computer vision, having practical applications in areas such as human-computer interaction, education, healthcare, and online monitoring. In this challenging FER task, there are three key issues especially prevalent: inter-class similarity, intra-class discrepancy, and scale sensitivity. While existing works typically address some of these issues, none have fully addressed all three challenges in a unified framework. In this paper, we propose a two-stream Pyramid crOss-fuSion TransformER network (POSTER), that aims to holistically solve all three issues. Specifically, we design a transformer-based cross-fusion method that enables effective collaboration of facial landmark features and image features to maximize proper attention to salient facial regions. Furthermore, POSTER employs a pyramid structure to promote scale invariance. Extensive experimental results demonstrate that our POSTER achieves new state-of-the-art results on RAF-DB (92.05\%), FERPlus (91.62\%), as well as AffectNet 7 class (67.31\%) and 8 class (63.34\%). The code is available at https://github.com/zczcwh/POSTER.},
	language = {en},
	urldate = {2024-08-22},
	journal = {arXiv.org},
	author = {Zheng, Ce and Mendieta, Matias and Chen, Chen},
	month = apr,
	year = {2022},
	keywords = {notion},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/HX9KSCNJ/Zheng et al. - 2022 - POSTER A Pyramid Cross-Fusion Transformer Network.pdf:application/pdf},
}

@inproceedings{li_multi-branch_2023,
	address = {Singapore, Singapore},
	title = {Multi-branch {Attention} {Consistency} {Network} for {Facial} {Expression} {Recognition}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350331820},
	url = {https://ieeexplore.ieee.org/document/10312661/},
	doi = {10.1109/IECON51785.2023.10312661},
	abstract = {Due to the high inter-class similarity and subjective annotation of facial expressions, annotation uncertainty has become the key challenge in recent years. In this paper, we propose a Multi-branch Attention Consistency Network for facial expression recognition by combining latent label distribution learning and attention consistency to alleviate the annotation uncertainty. To be specific, we design three modules, namely multi-branch feature classification (MFC), multi-branch latent distribution learning (MLD) and multi-class attention consistency (MAC). The MFC classifies uncertain expressions through multiple auxiliary branches, which obtains attention maps and the degree of confidence for different facial categories. The MLD guides the target branch to learn latent label distributions from auxiliary branches. The MAC learns attention regions by multi-class attention consistency between auxiliary and target branches. Finally, we demonstrate the effectiveness of our proposed method by conducting experiments on three popular facial expression datasets. Experimental results show that our method achieves the state-of-the-art results of 90.16\%, 89.98\%, 63.12\% accuracy on RAF-DB, FERPlus and AffectNet datasets, respectively.},
	language = {en},
	urldate = {2024-08-22},
	booktitle = {{IECON} 2023- 49th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	publisher = {IEEE},
	author = {Li, Jing and Hu, Tianyu and Ouyang, Gaoxiang},
	month = oct,
	year = {2023},
	keywords = {notion},
	pages = {1--6},
	file = {Li et al. - 2023 - Multi-branch Attention Consistency Network for Fac.pdf:/Users/aaronmac/Zotero/storage/WLJD82CC/Li et al. - 2023 - Multi-branch Attention Consistency Network for Fac.pdf:application/pdf},
}

@misc{wu_-net_2023,
	title = {{LA}-{Net}: {Landmark}-{Aware} {Learning} for {Reliable} {Facial} {Expression} {Recognition} under {Label} {Noise}},
	shorttitle = {{LA}-{Net}},
	url = {http://arxiv.org/abs/2307.09023},
	doi = {10.48550/arXiv.2307.09023},
	abstract = {Facial expression recognition (FER) remains a challenging task due to the ambiguity of expressions. The derived noisy labels significantly harm the performance in real-world scenarios. To address this issue, we present a new FER model named Landmark-Aware Net{\textasciitilde}(LA-Net), which leverages facial landmarks to mitigate the impact of label noise from two perspectives. Firstly, LA-Net uses landmark information to suppress the uncertainty in expression space and constructs the label distribution of each sample by neighborhood aggregation, which in turn improves the quality of training supervision. Secondly, the model incorporates landmark information into expression representations using the devised expression-landmark contrastive loss. The enhanced expression feature extractor can be less susceptible to label noise. Our method can be integrated with any deep neural network for better training supervision without introducing extra inference costs. We conduct extensive experiments on both in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net achieves state-of-the-art performance.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Wu, Zhiyu and Cui, Jinshi},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09023 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/WXDGJAXU/Wu and Cui - 2023 - LA-Net Landmark-Aware Learning for Reliable Facia.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/PIYIA8RT/2307.html:text/html},
}

@misc{zhang_leave_2023,
	title = {Leave {No} {Stone} {Unturned}: {Mine} {Extra} {Knowledge} for {Imbalanced} {Facial} {Expression} {Recognition}},
	shorttitle = {Leave {No} {Stone} {Unturned}},
	url = {http://arxiv.org/abs/2310.19636},
	doi = {10.48550/arXiv.2310.19636},
	abstract = {Facial expression data is characterized by a significant imbalance, with most collected data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our aim is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by that, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Zhang, Yuhang and Li, Yaqi and Qin, Lixiong and Liu, Xuannan and Deng, Weihong},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19636 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/B3MHJXKN/Zhang et al. - 2023 - Leave No Stone Unturned Mine Extra Knowledge for .pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/BDY5DYUX/2310.html:text/html},
}

@misc{mao_poster_2023,
	title = {{POSTER}++: {A} simpler and stronger facial expression recognition network},
	shorttitle = {{POSTER}++},
	url = {http://arxiv.org/abs/2301.12149},
	doi = {10.48550/arXiv.2301.12149},
	abstract = {Facial expression recognition (FER) plays an important role in a variety of real-world applications such as human-computer interaction. POSTER achieves the state-of-the-art (SOTA) performance in FER by effectively combining facial landmark and image features through two-stream pyramid cross-fusion design. However, the architecture of POSTER is undoubtedly complex. It causes expensive computational costs. In order to relieve the computational pressure of POSTER, in this paper, we propose POSTER++. It improves POSTER in three directions: cross-fusion, two-stream, and multi-scale feature extraction. In cross-fusion, we use window-based cross-attention mechanism replacing vanilla cross-attention mechanism. We remove the image-to-landmark branch in the two-stream design. For multi-scale feature extraction, POSTER++ combines images with landmark's multi-scale features to replace POSTER's pyramid design. Extensive experiments on several standard datasets show that our POSTER++ achieves the SOTA FER performance with the minimum computational cost. For example, POSTER++ reached 92.21\% on RAF-DB, 67.49\% on AffectNet (7 cls) and 63.77\% on AffectNet (8 cls), respectively, using only 8.4G floating point operations (FLOPs) and 43.7M parameters (Param). This demonstrates the effectiveness of our improvements.},
	urldate = {2024-08-25},
	publisher = {arXiv},
	author = {Mao, Jiawei and Xu, Rui and Yin, Xuesong and Chang, Yuanqi and Nie, Binling and Huang, Aibin},
	month = feb,
	year = {2023},
	note = {arXiv:2301.12149 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/PIRW32MZ/Mao et al. - 2023 - POSTER++ A simpler and stronger facial expression.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/77XHUI4N/2301.html:text/html},
}

@misc{shi_learning_2021,
	title = {Learning to {Amend} {Facial} {Expression} {Representation} via {De}-albino and {Affinity}},
	url = {http://arxiv.org/abs/2103.10189},
	doi = {10.48550/arXiv.2103.10189},
	abstract = {Facial Expression Recognition (FER) is a classification task that points to face variants. Hence, there are certain affinity features between facial expressions, receiving little attention in the FER literature. Convolution padding, despite helping capture the edge information, causes erosion of the feature map simultaneously. After multi-layer filling convolution, the output feature map named albino feature definitely weakens the representation of the expression. To tackle these challenges, we propose a novel architecture named Amending Representation Module (ARM). ARM is a substitute for the pooling layer. Theoretically, it can be embedded in the back end of any network to deal with the Padding Erosion. ARM efficiently enhances facial expression representation from two different directions: 1) reducing the weight of eroded features to offset the side effect of padding, and 2) decomposing facial features to simplify representation learning. Experiments on public benchmarks prove that our ARM boosts the performance of FER remarkably. The validation accuracies are respectively 90.42\% on RAF-DB, 65.2\% on Affect-Net, and 58.71\% on SFEW, exceeding current state-of-the-art methods. Our implementation and trained models are available at https://github.com/JiaweiShiCV/Amend-Representation-Module.},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Shi, Jiawei and Zhu, Songhao and Liang, Zhiwei},
	month = oct,
	year = {2021},
	note = {arXiv:2103.10189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/TL2DFU2K/Shi et al. - 2021 - Learning to Amend Facial Expression Representation.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/76MJI2MQ/2103.html:text/html},
}

@misc{wang_survey_2024,
	title = {A {Survey} on {Facial} {Expression} {Recognition} of {Static} and {Dynamic} {Emotions}},
	url = {http://arxiv.org/abs/2408.15777},
	doi = {10.48550/arXiv.2408.15777},
	abstract = {Facial expression recognition (FER) aims to analyze emotional states from static images and dynamic sequences, which is pivotal in enhancing anthropomorphic communication among humans, robots, and digital avatars by leveraging AI technologies. As the FER field evolves from controlled laboratory environments to more complex in-the-wild scenarios, advanced methods have been rapidly developed and new challenges and apporaches are encounted, which are not well addressed in existing reviews of FER. This paper offers a comprehensive survey of both image-based static FER (SFER) and video-based dynamic FER (DFER) methods, analyzing from model-oriented development to challenge-focused categorization. We begin with a critical comparison of recent reviews, an introduction to common datasets and evaluation criteria, and an in-depth workflow on FER to establish a robust research foundation. We then systematically review representative approaches addressing eight main challenges in SFER (such as expression disturbance, uncertainties, compound emotions, and cross-domain inconsistency) as well as seven main challenges in DFER (such as key frame sampling, expression intensity variations, and cross-modal alignment). Additionally, we analyze recent advancements, benchmark performances, major applications, and ethical considerations. Finally, we propose five promising future directions and development trends to guide ongoing research. The project page for this paper can be found at https://github.com/wangyanckxx/SurveyFER.},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Wang, Yan and Yan, Shaoqi and Liu, Yang and Song, Wei and Liu, Jing and Chang, Yang and Mai, Xinji and Hu, Xiping and Zhang, Wenqiang and Gan, Zhongxue},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15777 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/PBPBXXQJ/Wang et al. - 2024 - A Survey on Facial Expression Recognition of Stati.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/5Y2NC62I/2408.html:text/html},
}

@misc{islam_recent_2023,
	title = {Recent {Advances} in {Vision} {Transformer}: {A} {Survey} and {Outlook} of {Recent} {Work}},
	shorttitle = {Recent {Advances} in {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2203.01536},
	doi = {10.48550/arXiv.2203.01536},
	abstract = {Vision Transformers (ViTs) are becoming more popular and dominating technique for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a demanding technique in computer vision, ViTs have been successfully solved various vision problems while focusing on long-range relationships. In this paper, we begin by introducing the fundamental concepts and background of the self-attention mechanism. Next, we provide a comprehensive overview of recent top-performing ViT methods describing in terms of strength and weakness, computational cost as well as training and testing dataset. We thoroughly compare the performance of various ViT algorithms and most representative CNN methods on popular benchmark datasets. Finally, we explore some limitations with insightful observations and provide further research direction. The project page along with the collections of papers are available at https://github.com/khawar512/ViT-Survey},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Islam, Khawar},
	month = oct,
	year = {2023},
	note = {arXiv:2203.01536 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/Users/aaronmac/Zotero/storage/8F28AM4I/Islam - 2023 - Recent Advances in Vision Transformer A Survey an.pdf:application/pdf;arXiv.org Snapshot:/Users/aaronmac/Zotero/storage/DL3YKFFG/2203.html:text/html},
}

@article{leon-sarkis_alisis_2017,
	title = {Un análisis comparativo de los algoritmos {Fast} {Radial} {Symmetry} {Transform} y {Hough} {Transform} para la detección automática de granos de café en imágenes},
	url = {https://repositoriotec.tec.ac.cr/handle/2238/9372},
	abstract = {In this work we present a strategy that contributes to the overall solution of a 
problem presented by the Costa Rica Co ee Institute (ICAFE). ICAFE owns a set 
of coffee grains images and needs to  nd an automatic way, through computer 
vision, to detect and count the number of grains in each image in order to increase 
the e ciency in the process of estimating yield. 
A strategy to detect co ee grains in images is proposed, by combining the 
algorithms Fast Radial Symmetry Transform[8] and Hough Transform[19]. Then, 
this strategy is incorporated in the grain detection process of P-TRAP[13], an 
open-source tool, to increase the precision in the detection of existing coffee grains. 
The images are taken with a mobile device in a non-controlled environment in which 
the grains are not pulled o  their natural environment. 
Likewise, a comparative analisis is done between the P-TRAP version developed in 
this study and both algorithms running individually. The number of existing grains 
in an image is determined manually. Then, the cherry detection process is executed 
over each image and results are collected. Finally, a detailed analisis is done over 
the results obtained.},
	language = {spa},
	urldate = {2024-10-07},
	author = {León-Sarkis, Marco},
	year = {2017},
	note = {Accepted: 2018-02-08T21:53:34Z
Publisher: Instituto Tecnológico de Costa Rica},
	keywords = {notion},
	file = {Full Text PDF:/Users/aaronmac/Zotero/storage/55R3297X/León-Sarkis - 2017 - Un análisis comparativo de los algoritmos Fast Rad.pdf:application/pdf},
}

@misc{noauthor_real-world_nodate,
	title = {Real-world {Affective} {Faces} ({RAF}) {Database}},
	url = {http://www.whdeng.cn/raf/model1.html},
	urldate = {2024-10-12},
	keywords = {notion},
	file = {Real-world Affective Faces (RAF) Database:/Users/aaronmac/Zotero/storage/CMQTIDD6/model1.html:text/html},
}

@misc{noauthor_facial_nodate,
	title = {Facial {Expression} {Recognition} with {Visual} {Transformers} and {Attentional} {Selective} {Fusion}},
	url = {https://ar5iv.labs.arxiv.org/html/2103.16854},
	abstract = {Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions.
Although substantial progresses have been…},
	language = {en},
	urldate = {2024-10-24},
	journal = {ar5iv},
	keywords = {notion},
	file = {Snapshot:/Users/aaronmac/Zotero/storage/2V57HG5D/2103.html:text/html},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {RAF}-{DB} {Dataset}},
	url = {https://paperswithcode.com/dataset/raf-db},
	abstract = {The Real-world Affective Faces Database (RAF-DB) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.},
	language = {en},
	urldate = {2024-10-26},
	keywords = {notion},
	file = {Snapshot:/Users/aaronmac/Zotero/storage/YMFJT7HU/raf-db.html:text/html},
}
